{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":59291,"databundleVersionId":6678907,"sourceType":"competition"},{"sourceId":13726189,"sourceType":"datasetVersion","datasetId":8732888}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T15:11:27.934630Z","iopub.execute_input":"2025-11-14T15:11:27.935476Z","iopub.status.idle":"2025-11-14T15:11:28.265206Z","shell.execute_reply.started":"2025-11-14T15:11:27.935449Z","shell.execute_reply":"2025-11-14T15:11:28.264434Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\n# 1. Install all three core libraries together, forcing the desired stable versions.\n!pip install numpy==1.26.4 scipy==1.15.3 scikit-learn==1.7.2 --upgrade --force-reinstall\n\nprint(\"\\nüîÑ Step 2: Installing XGBoost without touching core dependencies...\")\n\n# 2. Install XGBoost, but critically, use --no-deps to prevent it from replacing NumPy.\n!pip install xgboost --upgrade --force-reinstall --no-deps\n\n# We also need to reinstall your TabPFN dependency since it might have been uninstalled\n# by one of the previous commands (and it was the original source of the sklearn error).\n# Note: You may need to replace this path if you changed it earlier.\n!pip install /kaggle/input/tabpfn-019-whl/tabpfn-0.1.9-py3-none-any.whl\n\nprint(\"\\n‚úÖ Stable toolchain installation complete. **RESTART THE NOTEBOOK KERNEL NOW!**\")\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T15:11:28.266500Z","iopub.execute_input":"2025-11-14T15:11:28.266875Z","iopub.status.idle":"2025-11-14T15:11:28.272593Z","shell.execute_reply.started":"2025-11-14T15:11:28.266849Z","shell.execute_reply":"2025-11-14T15:11:28.271990Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 1. Environment Setup","metadata":{}},{"cell_type":"markdown","source":"## Import Packages","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport gc\nimport ctypes\nimport os\nimport itertools\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nimport random\nimport pprint\nimport time\nimport copy\nimport lightgbm as lgb\nimport torch\nimport polars as pl\nimport optuna\n\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LinearRegression,Lasso, Ridge, ElasticNet\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error,accuracy_score\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.preprocessing import LabelEncoder, PowerTransformer, RobustScaler, FunctionTransformer\nfrom sklearn.model_selection import KFold, StratifiedKFold, GroupKFold, train_test_split\nfrom sklearn.decomposition import TruncatedSVD, PCA\nfrom sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.cluster import KMeans\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.impute import SimpleImputer\nfrom sklearn import metrics\n%matplotlib inline\nfrom random import choice, choices\nfrom functools import reduce, partial\nfrom tqdm import tqdm\nfrom itertools import cycle\nfrom collections import Counter\nfrom scipy import stats\nfrom scipy.stats import skew, kurtosis\nfrom transformers import BertTokenizer\nfrom collections import Counter, defaultdict\nfrom tqdm.autonotebook import tqdm\nfrom math import sqrt\nfrom sklearn import model_selection\n\ndef clean_memory():\n    gc.collect()\n    ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n    torch.cuda.empty_cache()\nclean_memory()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T15:11:28.273382Z","iopub.execute_input":"2025-11-14T15:11:28.273602Z","iopub.status.idle":"2025-11-14T15:11:39.080815Z","shell.execute_reply.started":"2025-11-14T15:11:28.273586Z","shell.execute_reply":"2025-11-14T15:11:39.080156Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Creating Dataframes of Kaggle Data","metadata":{}},{"cell_type":"code","source":"# Define paths to CSV files\nTRAIN_LOGS   = \"/kaggle/input/linking-writing-processes-to-writing-quality/train_logs.csv\"\nTRAIN_SCORES = \"/kaggle/input/linking-writing-processes-to-writing-quality/train_scores.csv\"\nTEST_LOGS    = \"/kaggle/input/linking-writing-processes-to-writing-quality/test_logs.csv\"\nSAMPLE_SUB   = \"/kaggle/input/linking-writing-processes-to-writing-quality/sample_submission.csv\"\n\ndf_train_logs = pd.read_csv(TRAIN_LOGS)\ndf_train_scores = pd.read_csv(TRAIN_SCORES)\ndf_test_logs = pd.read_csv(TEST_LOGS)\ndf_sample_submission = pd.read_csv(SAMPLE_SUB)\n\nprint(\"Train logs:\", TRAIN_LOGS)\nprint(\"Train scores:\", TRAIN_SCORES)\nprint(\"Test logs:\", TEST_LOGS)\nprint(\"Sample submission:\", SAMPLE_SUB)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T15:11:39.082112Z","iopub.execute_input":"2025-11-14T15:11:39.082759Z","iopub.status.idle":"2025-11-14T15:11:50.730916Z","shell.execute_reply.started":"2025-11-14T15:11:39.082735Z","shell.execute_reply":"2025-11-14T15:11:50.730155Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"## Analysis (Function)","metadata":{}},{"cell_type":"code","source":"def analyse_data(df_orig):\n    \"\"\"\n    Analyzes a dataframe for data quality and structure.\n    Prints compact, useful diagnostics ‚Äî avoids unnecessary verbosity.\n    \"\"\"\n    df = df_orig.copy()\n    print(\"üìä ANALYSING DATAFRAME\\n\")\n\n    # 1Ô∏è‚É£ Missing values summary\n    na_counts = df.isna().sum()\n    total_missing = na_counts.sum()\n    if total_missing > 0:\n        print(f\"üî∏ Missing values detected in {sum(na_counts > 0)} / {len(df.columns)} columns\")\n        print(na_counts[na_counts > 0].sort_values(ascending=False))\n    else:\n        print(\"‚úÖ No missing values found.\")\n    \n    # 2Ô∏è‚É£ Data type consistency check\n    print(\"\\nüß© Checking for inconsistent data types...\")\n    inconsistent_cols = []\n    for column in df.columns:\n        types = df[column].apply(type)\n        majority_type = types.mode()[0]\n        anomaly_mask = types != majority_type\n        if anomaly_mask.any():\n            inconsistent_cols.append(column)\n            num_anomalies = anomaly_mask.sum()\n            print(f\"‚ö†Ô∏è  {column}: {num_anomalies} anomalous entries (expected {majority_type.__name__})\")\n    if not inconsistent_cols:\n        print(\"‚úÖ All columns have consistent data types.\")\n\n    # 3Ô∏è‚É£ Negative numeric values\n    numeric_cols = df.select_dtypes(include=[\"number\"])\n    neg_mask = (numeric_cols < 0).any()\n    neg_cols = neg_mask[neg_mask].index.tolist()\n    if neg_cols:\n        print(f\"\\n‚ö†Ô∏è Columns with negative values ({len(neg_cols)}): {neg_cols}\")\n    else:\n        print(\"\\n‚úÖ No negative values in numeric columns.\")\n\n    # 4Ô∏è‚É£ Distinct value counts\n    nunique = df.nunique()\n    print(\"\\nüì¶ Distinct values summary:\")\n    print(nunique.describe()[['min', 'max']])\n    # Only show top 10 most unique columns\n    top_unique = nunique.sort_values(ascending=False).head(10)\n    print(\"üîπ Top 10 columns by unique count:\")\n    print(top_unique)\n\n    # 5Ô∏è‚É£ Sample string columns (only small samples)\n    obj_cols = df.select_dtypes(include=[\"object\"]).columns\n    if len(obj_cols) > 0:\n        print(\"\\nüìù Sample entries from text columns:\")\n        for col in obj_cols:\n            unique_vals = df[col].dropna().unique()\n            sample_count = min(len(unique_vals), 5)\n            print(f\"‚Ä¢ {col}: {unique_vals[:sample_count]}\")\n    else:\n        print(\"\\n‚úÖ No object/string columns found.\")\n\n    # ‚úÖ Final summary\n    print(\"\\nüìã Summary:\")\n    print(f\"Rows: {df.shape[0]} | Columns: {df.shape[1]}\")\n    print(\"Analysis complete.\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T15:11:50.731877Z","iopub.execute_input":"2025-11-14T15:11:50.732081Z","iopub.status.idle":"2025-11-14T15:11:50.741274Z","shell.execute_reply.started":"2025-11-14T15:11:50.732064Z","shell.execute_reply":"2025-11-14T15:11:50.740294Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Transformation (Function)","metadata":{}},{"cell_type":"code","source":"def transform_data(df_orig):\n    \"\"\"\n    Applies transformation steps to activity, event, and text_change columns.\n    Prints only one example entry per stage for verification.\n    \"\"\"\n    import re\n    import numpy as np\n    import pandas as pd\n\n    df = df_orig.copy()\n    print(\"üîß Transforming dataset...\")\n\n    # ==========================================================\n    # 1Ô∏è‚É£ Transform 'activity' column\n    # ==========================================================\n    def calculate_move_distance(activity):\n        move_pattern = r'Move From \\[(-?\\d+), (-?\\d+)\\] To \\[(-?\\d+), (-?\\d+)\\]'\n        match = re.match(move_pattern, activity)\n        if match:\n            x1, y1, x2, y2 = map(int, match.groups())\n            distance = np.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2)\n            return f\"move_{int(round(distance))}\"\n        else:\n            return activity\n\n    df[\"activity_trf\"] = df[\"activity\"].apply(calculate_move_distance)\n    df[\"activity_trf\"] = df[\"activity_trf\"].replace({\"Remove/Cut\": \"Cut\"})\n\n    print(\"\\n‚úÖ Sample transformed 'activity_trf':\")\n    display(df[[\"activity\", \"activity_trf\"]].head(1))\n\n    # ==========================================================\n    # 2Ô∏è‚É£ Transform 'down_event' and 'up_event' columns\n    # ==========================================================\n    def transform_event(event):\n        event_str = str(event)\n        if len(event_str) == 1 and event_str.isalnum():\n            return \"q\"\n        return event\n\n    df[\"down_event_trf\"] = df[\"down_event\"].apply(transform_event)\n    df[\"up_event_trf\"] = df[\"up_event\"].apply(transform_event)\n\n    print(\"‚úÖ Sample transformed 'down_event' & 'up_event':\")\n    display(df[[\"down_event\", \"down_event_trf\", \"up_event\", \"up_event_trf\"]].head(1))\n\n    # Quick distinct summary (short)\n    distinct_counts = df[[\"down_event_trf\", \"up_event_trf\"]].nunique()\n    print(\"Distinct transformed event types:\")\n    print(distinct_counts.to_dict())\n\n    # ==========================================================\n    # 3Ô∏è‚É£ Transform 'text_change' column\n    # ==========================================================\n    def parse_text_change(val):\n        val = str(val).replace(\" \", \"space\")\n        if \"q\" not in val:\n            return val\n        elif \"=>\" in val:\n            before, after = val.split(\"=>\", 1)\n            delta = len(after.strip()) - len(before.strip())\n            if delta > 0:\n                return f\"q_add_{delta}\"\n            elif delta < 0:\n                return f\"q_subtract_{abs(delta)}\"\n            else:\n                return \"q_0\"\n        else:\n            delta = len(val.strip())\n            return f\"q_add_{delta}\" if delta > 0 else \"q_0\"\n\n    df[\"text_change_trf\"] = df[\"text_change\"].apply(parse_text_change)\n\n    print(\"\\n‚úÖ Sample transformed 'text_change_trf':\")\n    display(df[[\"text_change\", \"text_change_trf\"]].head(1))\n\n    # ==========================================================\n    # ‚úÖ Final Summary\n    # ==========================================================\n    print(\"\\nüìã Transformation complete!\")\n    print(f\"Rows: {df.shape[0]} | Columns: {df.shape[1]}\")\n    print(f\"New columns added: activity_trf, down_event_trf, up_event_trf, text_change_trf\\n\")\n\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T15:11:50.742130Z","iopub.execute_input":"2025-11-14T15:11:50.742410Z","iopub.status.idle":"2025-11-14T15:11:50.764632Z","shell.execute_reply.started":"2025-11-14T15:11:50.742385Z","shell.execute_reply":"2025-11-14T15:11:50.763944Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Clean (Function)","metadata":{}},{"cell_type":"code","source":"def clean_data(df_orig):\n    \"\"\"\n    Cleans string-type columns in a DataFrame:\n      - Converts text to lowercase\n      - Strips leading/trailing spaces\n    Prints one example row for verification after cleaning.\n    \"\"\"\n    import pandas as pd\n\n    df = df_orig.copy()\n    print(\"üßπ Cleaning data...\")\n\n    obj_cols = df.select_dtypes(include=[\"object\"]).columns.tolist()\n    if not obj_cols:\n        print(\"‚úÖ No object-type columns found ‚Äî nothing to clean.\")\n        return df\n\n    # Apply transformations\n    for col in obj_cols:\n        df[col] = df[col].astype(str).str.lower().str.strip()\n\n    # Show one sample row to confirm cleaning\n    print(f\"‚úÖ Cleaned {len(obj_cols)} text columns.\")\n    print(\"üìã Sample after cleaning:\")\n    display(df[obj_cols].head(1))\n\n    print(f\"Rows: {df.shape[0]} | Columns: {df.shape[1]}\\n\")\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T15:11:50.765259Z","iopub.execute_input":"2025-11-14T15:11:50.765432Z","iopub.status.idle":"2025-11-14T15:11:50.784580Z","shell.execute_reply.started":"2025-11-14T15:11:50.765418Z","shell.execute_reply":"2025-11-14T15:11:50.783828Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Aggregation (Function)","metadata":{}},{"cell_type":"code","source":"def aggregate_data(df_orig):\n    \"\"\"\n    Aggregate raw writing process logs into essay-level behavioral features.\n    Prints concise progress info and shows one sample entry after each major block.\n    \"\"\"\n    import numpy as np\n    import pandas as pd\n\n    print(\"üßÆ Aggregating essay-level behavioral features...\")\n\n    # ==========================================================\n    # 1Ô∏è‚É£ SORT & GROUP\n    # ==========================================================\n    df = df_orig.copy()\n    df_sorted = df.sort_values(by=[\"id\", \"event_id\"]).reset_index(drop=True)\n    g = df_sorted.groupby(\"id\")\n    out = pd.DataFrame()\n\n    print(\"‚úÖ Data sorted and grouped by 'id'.\")\n    print(f\"Rows: {df_sorted.shape[0]} | Columns: {df_sorted.shape[1]}\")\n\n    # ==========================================================\n    # 2Ô∏è‚É£ BASIC EVENT FEATURES\n    # ==========================================================\n    out[\"total_events\"] = g[\"event_id\"].count()\n    out[\"writing_start\"] = g[\"down_time\"].min()\n    out[\"writing_end\"] = g[\"up_time\"].max()\n    out[\"total_time_spent_on_essay\"] = out[\"writing_end\"] - out[\"writing_start\"]\n    out[\"mean_action_time\"] = g[\"action_time\"].mean()\n    out[\"sum_action_time\"] = g[\"action_time\"].sum()\n\n    print(\"üïí Computed basic timing and event features.\")\n    display(out.head(1))\n\n    # ==========================================================\n    # 3Ô∏è‚É£ ACTIVITY TRANSFORM (MOVE VS NON-MOVE)\n    # ==========================================================\n    move_condition = df_sorted['activity_trf'].str.contains(\"move\", case=False, na=False)\n    out[\"non_move_count\"] = (~move_condition).groupby(df_sorted['id']).sum()\n    out[\"move_count\"] = move_condition.groupby(df_sorted['id']).sum()\n\n    non_move_activity_counts = df_sorted.loc[~move_condition, 'activity_trf'].unique()\n    for activity in non_move_activity_counts:\n        out[f\"count_{activity}\"] = g[\"activity_trf\"].apply(lambda x: (x == activity).sum())\n\n    print(f\"üß≠ Added move/non-move activity stats ({len(non_move_activity_counts)} activity types).\")\n    display(out.head(1))\n\n    # ==========================================================\n    # 4Ô∏è‚É£ MOVE DISTANCE STATS\n    # ==========================================================\n    df_sorted[\"move_distance\"] = (\n        df_sorted[\"activity_trf\"].str.extract(r\"move_(\\d+)\").astype(float)\n    )\n    move_distance_stats = g[\"move_distance\"].agg(\n        sum_move_distance=\"sum\",\n        mean_move_distance=\"mean\"\n    )\n    out = out.merge(move_distance_stats, on=\"id\", how=\"left\")\n    out.loc[out[\"move_count\"] == 0, \"mean_move_distance\"] = 0\n    out.fillna({\"sum_move_distance\": 0, \"mean_move_distance\": 0}, inplace=True)\n\n    print(\"üìè Computed move distance statistics.\")\n    display(out.head(1))\n\n    # ==========================================================\n    # 5Ô∏è‚É£ TEXT CHANGE DYNAMICS\n    # ==========================================================\n    out[\"non_q_tc_count\"] = g[\"text_change_trf\"].apply(lambda s: (~s.str.startswith(\"q\")).sum())\n    out[\"q_tc_count\"] = g[\"text_change_trf\"].apply(lambda s: s.str.startswith(\"q\").sum())\n\n    def parse_q_change(val):\n        if isinstance(val, str):\n            if val.startswith(\"q_add_\"):\n                return int(val.split(\"_\")[-1])\n            elif val.startswith(\"q_subtract_\"):\n                return -int(val.split(\"_\")[-1])\n        return 0\n\n    df_sorted[\"q_delta\"] = df_sorted[\"text_change_trf\"].apply(parse_q_change)\n    out[\"q_overall_delta\"] = g[\"q_delta\"].sum()\n\n    print(\"‚úèÔ∏è Extracted text-change and q-delta features.\")\n    display(out.head(1))\n\n    # ==========================================================\n    # 6Ô∏è‚É£ CURSOR + WORD COUNT STATS\n    # ==========================================================\n    out[\"mean_cursor\"] = g[\"cursor_position\"].mean()\n    out[\"std_cursor\"] = g[\"cursor_position\"].std()\n    out[\"max_cursor\"] = g[\"cursor_position\"].max()\n\n    wc_first = g[\"word_count\"].first()\n    wc_last = g[\"word_count\"].last()\n    out[\"final_word_count\"] = wc_last\n    out[\"max_word_count\"] = g[\"word_count\"].max()\n    out[\"min_word_count\"] = g[\"word_count\"].min()\n    out[\"std_word_count\"] = g[\"word_count\"].std()\n\n    print(\"üñ±Ô∏è Added cursor and word count stats.\")\n    display(out.head(1))\n\n    # ==========================================================\n    # 7Ô∏è‚É£ DERIVED BEHAVIORAL RATIOS\n    # ==========================================================\n    out[\"words_per_event\"] = out[\"final_word_count\"] / out[\"total_events\"].clip(lower=1)\n    out[\"words_per_second\"] = out[\"final_word_count\"] / out[\"total_time_spent_on_essay\"].clip(lower=1)\n\n    out[\"edit_intensity\"] = (\n        out.get(\"count_cut\", 0) + out.get(\"count_replace\", 0) + out.get(\"count_nonproduction\", 0)\n    ) / out[\"total_events\"].clip(lower=1)\n\n    out[\"revision_ratio\"] = (\n        out.get(\"count_cut\", 0) + out.get(\"count_replace\", 0)\n    ) / (out.get(\"count_input\", 1) + 1)\n\n    out[\"net_char_change_ratio\"] = out[\"q_overall_delta\"] / out[\"final_word_count\"].clip(lower=1)\n    out[\"q_activity_ratio\"] = (\n        (out.get(\"q_tc_count\", 0) + out.get(\"non_q_tc_count\", 0)) / out[\"total_events\"].clip(lower=1)\n    )\n\n    out[\"cursor_movement_intensity\"] = out[\"sum_move_distance\"] / out[\"total_events\"].clip(lower=1)\n    out[\"avg_move_distance\"] = out.get(\"mean_move_distance\", 0)\n    out[\"word_var_ratio\"] = out[\"std_word_count\"] / out[\"final_word_count\"].clip(lower=1)\n    out[\"time_per_word\"] = out[\"total_time_spent_on_essay\"] / out[\"final_word_count\"].clip(lower=1)\n    out[\"time_per_event\"] = out[\"total_time_spent_on_essay\"] / out[\"total_events\"].clip(lower=1)\n\n    out.replace([np.inf, -np.inf], 0, inplace=True)\n    out.fillna(0, inplace=True)\n\n    # Ensure ID is a column, not index\n    if out.index.name == \"id\":\n        out.reset_index(inplace=True)\n\n    print(\"‚öôÔ∏è Derived higher-level behavioral ratios.\")\n    display(out.head(1))\n\n    # ==========================================================\n    # ‚úÖ SUMMARY\n    # ==========================================================\n    print(\"\\n‚úÖ Aggregation complete!\")\n    print(f\"Final shape: {out.shape[0]} rows √ó {out.shape[1]} columns\")\n\n    return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T15:11:50.785257Z","iopub.execute_input":"2025-11-14T15:11:50.785459Z","iopub.status.idle":"2025-11-14T15:11:50.804911Z","shell.execute_reply.started":"2025-11-14T15:11:50.785442Z","shell.execute_reply":"2025-11-14T15:11:50.804218Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================================================\n#  STEP 1 ‚Üí ANALYSE RAW DATA\n# ==========================================================\nprint(\"Step 1: Analysing data...\")\ndf_train_logs_analysis = df_train_logs.copy()\nanalyse_data(df_train_logs_analysis)\nprint(f\"‚Üí Shape after Step 1: {df_train_logs_analysis.shape}\")\n\n# ==========================================================\n#  STEP 2 ‚Üí TRANSFORM DATA\n# ==========================================================\nprint(\"\\nStep 2: Transforming columns...\")\ndf_train_logs_transformed = transform_data(df_train_logs_analysis.copy())\nprint(f\"‚Üí Shape after Step 2: {df_train_logs_transformed.shape}\")\n\n# ==========================================================\n#  STEP 3 ‚Üí CLEAN DATA\n# ==========================================================\nprint(\"\\nStep 3: Cleaning data...\")\ndf_train_logs_cleaned = clean_data(df_train_logs_transformed.copy())\nprint(f\"‚Üí Shape after Step 3: {df_train_logs_cleaned.shape}\")\n\n# ==========================================================\n#  STEP 4 ‚Üí AGGREGATE EVENT-LEVEL FEATURES (Essay-Level)\n# ==========================================================\nprint(\"\\nStep 4: Aggregating event-level features...\")\ndf_train_agg_logs = aggregate_data(df_train_logs_cleaned.copy())\nprint(f\"‚Üí Shape after Step 4: {df_train_agg_logs.shape}\")\n\n# ==========================================================\n#  FINAL SUMMARY\n# ==========================================================\nprint(\"\\n‚úÖ Preprocessing pipeline completed successfully!\")\nprint(f\"Final dataset shape: {df_train_agg_logs.shape}\")\ndisplay(df_train_agg_logs.head(3))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T15:11:50.805822Z","iopub.execute_input":"2025-11-14T15:11:50.806019Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================================================\n#  STEP 1 ‚Üí ANALYSE RAW DATA (Test Set)\n# ==========================================================\nprint(\"Step 1: Analysing data...\")\ndf_test_logs_analysis = df_test_logs.copy()\nanalyse_data(df_test_logs_analysis)\nprint(f\"‚Üí Shape after Step 1: {df_test_logs_analysis.shape}\")\n\n# ==========================================================\n#  STEP 2 ‚Üí TRANSFORM DATA (Test Set)\n# ==========================================================\nprint(\"\\nStep 2: Transforming columns...\")\ndf_test_logs_transformed = transform_data(df_test_logs_analysis.copy())\nprint(f\"‚Üí Shape after Step 2: {df_test_logs_transformed.shape}\")\n\n# ==========================================================\n#  STEP 3 ‚Üí CLEAN DATA (Test Set)\n# ==========================================================\nprint(\"\\nStep 3: Cleaning data...\")\ndf_test_logs_cleaned = clean_data(df_test_logs_transformed.copy())\nprint(f\"‚Üí Shape after Step 3: {df_test_logs_cleaned.shape}\")\n\n# ==========================================================\n#  STEP 4 ‚Üí AGGREGATE EVENT-LEVEL FEATURES (Essay-Level) (Test Set)\n# ==========================================================\nprint(\"\\nStep 4: Aggregating event-level features...\")\ndf_test_agg_logs = aggregate_data(df_test_logs_cleaned.copy())\nprint(f\"‚Üí Shape after Step 4: {df_test_agg_logs.shape}\")\n\n# ==========================================================\n#  FINAL SUMMARY (Test Set)\n# ==========================================================\nprint(\"\\n‚úÖ Preprocessing pipeline completed successfully for the test set!\")\nprint(f\"Final dataset shape: {df_test_agg_logs.shape}\")\ndisplay(df_test_agg_logs.head(3))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Reconstruction (Function)","metadata":{}},{"cell_type":"code","source":"import textwrap\nfrom tqdm import tqdm\nimport pandas as pd\n\ndef getEssays(df, show_first=True):\n    \"\"\"\n    Reconstructs full essay texts from event-level logs.\n    Returns a Series indexed by essay IDs.\n    \"\"\"\n    text_df = df[['id', 'activity', 'cursor_position', 'text_change']].copy()\n    text_df = text_df[text_df.activity != 'Nonproduction']\n    grouped = text_df.groupby('id', sort=False)\n\n    essays = {}\n\n    print(f\"üß† Reconstructing {len(grouped)} essays...\")\n    for essay_id, group in tqdm(grouped, total=len(grouped), desc=\"Processing essays\"):\n        essay_text = \"\"\n        group = group[['activity', 'cursor_position', 'text_change']].values\n\n        for activity, cursor_pos, text_change in group:\n            if activity == 'Replace':\n                before, after = text_change.split(' => ')\n                essay_text = essay_text[:cursor_pos - len(after)] + after + essay_text[cursor_pos - len(after) + len(before):]\n                continue\n            if activity == 'Paste':\n                essay_text = essay_text[:cursor_pos - len(text_change)] + text_change + essay_text[cursor_pos - len(text_change):]\n                continue\n            if activity == 'Remove/Cut':\n                essay_text = essay_text[:cursor_pos] + essay_text[cursor_pos + len(text_change):]\n                continue\n            if \"Move\" in activity:\n                cropped = activity[10:]\n                start, end = [seg.split(', ') for seg in cropped.split(' To ')]\n                move_data = (int(start[0][1:]), int(start[1][:-1]),\n                             int(end[0][1:]), int(end[1][:-1]))\n                if move_data[0] != move_data[2]:\n                    if move_data[0] < move_data[2]:\n                        essay_text = essay_text[:move_data[0]] + essay_text[move_data[1]:move_data[3]] + essay_text[move_data[0]:move_data[1]] + essay_text[move_data[3]:]\n                    else:\n                        essay_text = essay_text[:move_data[2]] + essay_text[move_data[0]:move_data[1]] + essay_text[move_data[2]:move_data[0]] + essay_text[move_data[1]:]\n                continue\n            essay_text = essay_text[:cursor_pos - len(text_change)] + text_change + essay_text[cursor_pos - len(text_change):]\n\n        essays[essay_id] = essay_text\n\n    essays_series = pd.Series(essays, name='essay_text')\n\n    # ‚úÖ Show only the first essay's text\n    if show_first and not essays_series.empty:\n        first_id = essays_series.index[0]\n        print(f\"\\nüìù First reconstructed essay (ID: {first_id}):\\n\")\n        print(textwrap.fill(essays_series.iloc[0][:1000], width=100))\n        print(\"\\n-----------------------------------------------\\n\")\n\n    return essays_series","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Silver Bullet","metadata":{}},{"cell_type":"code","source":"import polars as pl\nimport pandas as pd\nimport numpy as np\nimport re\nfrom lightgbm import LGBMRegressor\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom scipy.stats import skew, kurtosis\nimport warnings\nfrom sklearn.metrics import mean_squared_error\nfrom tqdm import tqdm\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_cols = ['down_time', 'up_time', 'action_time', 'cursor_position', 'word_count']\nactivities = ['Input', 'Remove/Cut', 'Nonproduction', 'Replace', 'Paste']\nevents = ['q', 'Space', 'Backspace', 'Shift', 'ArrowRight', 'Leftclick', 'ArrowLeft', '.', ',', 'ArrowDown', 'ArrowUp', 'Enter', 'CapsLock', \"'\", 'Delete', 'Unidentified']\ntext_changes = ['q', ' ', '.', ',', '\\n', \"'\", '\"', '-', '?', ';', '=', '/', '\\\\', ':']\n\n\ndef count_by_values(df, colname, values):\n    fts = df.select(pl.col('id').unique(maintain_order=True))\n    for i, value in enumerate(values):\n        tmp_df = df.group_by('id').agg(pl.col(colname).is_in([value]).sum().alias(f'{colname}_{i}_cnt'))\n        fts  = fts.join(tmp_df, on='id', how='left') \n    return fts\n\n\ndef dev_feats(df):\n    \n    print(\"< Count by values features >\")\n    \n    feats = count_by_values(df, 'activity', activities)\n    feats = feats.join(count_by_values(df, 'text_change', text_changes), on='id', how='left') \n    feats = feats.join(count_by_values(df, 'down_event', events), on='id', how='left') \n    feats = feats.join(count_by_values(df, 'up_event', events), on='id', how='left') \n\n    print(\"< Input words stats features >\")\n\n    temp = df.filter((~pl.col('text_change').str.contains('=>')) & (pl.col('text_change') != 'NoChange'))\n    temp = temp.group_by('id').agg(pl.col('text_change').str.concat('').str.extract_all(r'q+'))\n    temp = temp.with_columns(\n        input_word_count = pl.col('text_change').list.len(), # changed from .lengths()\n        input_word_length_mean = pl.col('text_change').map_elements(lambda x: np.mean([len(i) for i in x] if len(x) > 0 else 0), return_dtype=pl.Float64), # changed from .apply()\n        input_word_length_max = pl.col('text_change').map_elements(lambda x: np.max([len(i) for i in x] if len(x) > 0 else 0), return_dtype=pl.Float64),\n        input_word_length_std = pl.col('text_change').map_elements(lambda x: np.std([len(i) for i in x] if len(x) > 0 else 0), return_dtype=pl.Float64),\n        input_word_length_median = pl.col('text_change').map_elements(lambda x: np.median([len(i) for i in x] if len(x) > 0 else 0), return_dtype=pl.Float64),\n        input_word_length_skew = pl.col('text_change').map_elements(lambda x: skew([len(i) for i in x] if len(x) > 0 else 0), return_dtype=pl.Float64)\n    )\n    temp = temp.drop('text_change')\n    feats = feats.join(temp, on='id', how='left') \n\n\n    \n    print(\"< Numerical columns features >\")\n\n    temp = df.group_by(\"id\").agg(\n        pl.sum('action_time').alias('action_time_sum'), # alias() instead of suffix()\n\n        *[pl.mean(c).alias(f'{c}_mean') for c in num_cols],\n        *[pl.std(c).alias(f'{c}_std') for c in num_cols],\n        *[pl.median(c).alias(f'{c}_median') for c in num_cols],\n        *[pl.min(c).alias(f'{c}_min') for c in num_cols],\n        *[pl.max(c).alias(f'{c}_max') for c in num_cols],\n        *[pl.quantile(c, 0.5).alias(f'{c}_quantile') for c in num_cols],\n    )\n    feats = feats.join(temp, on='id', how='left') \n\n\n    print(\"< Categorical columns features >\")\n    \n    temp  = df.group_by(\"id\").agg(pl.n_unique(['activity', 'down_event', 'up_event', 'text_change']))\n    feats = feats.join(temp, on='id', how='left') \n\n\n    \n    print(\"< Idle time features >\")\n\n    temp = df.with_columns(pl.col('up_time').shift().over('id').alias('up_time_lagged'))\n    temp = temp.with_columns((abs(pl.col('down_time') - pl.col('up_time_lagged')) / 1000).fill_null(0).alias('time_diff'))\n    temp = temp.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n    temp = temp.group_by(\"id\").agg(inter_key_largest_lantency = pl.max('time_diff'),\n                                   inter_key_median_lantency = pl.median('time_diff'),\n                                   mean_pause_time = pl.mean('time_diff'),\n                                   std_pause_time = pl.std('time_diff'),\n                                   total_pause_time = pl.sum('time_diff'),\n                                   pauses_half_sec = pl.col('time_diff').filter((pl.col('time_diff') > 0.5) & (pl.col('time_diff') < 1)).count(),\n                                   pauses_1_sec = pl.col('time_diff').filter((pl.col('time_diff') > 1) & (pl.col('time_diff') < 1.5)).count(),\n                                   pauses_1_half_sec = pl.col('time_diff').filter((pl.col('time_diff') > 1.5) & (pl.col('time_diff') < 2)).count(),\n                                   pauses_2_sec = pl.col('time_diff').filter((pl.col('time_diff') > 2) & (pl.col('time_diff') < 3)).count(),\n                                   pauses_3_sec = pl.col('time_diff').filter(pl.col('time_diff') > 3).count(),)\n    feats = feats.join(temp, on='id', how='left') \n    \n    print(\"< P-bursts features >\")\n\n    temp = df.with_columns(pl.col('up_time').shift().over('id').alias('up_time_lagged'))\n    temp = temp.with_columns((abs(pl.col('down_time') - pl.col('up_time_lagged')) / 1000).fill_null(0).alias('time_diff'))\n    temp = temp.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n    temp = temp.with_columns((pl.col(\"time_diff\") < 2).alias(\"is_burst\"))\n    temp = temp.with_columns(pl.col(\"is_burst\").rle_id().alias(\"burst_id\"))\n    temp = temp.with_columns(pl.count().over(\"burst_id\").alias(\"P-bursts\"))\n    temp = temp.filter(pl.col(\"is_burst\") == True)\n    temp = temp.drop_nulls()\n    temp = temp.group_by(\"id\").agg(\n        pl.mean(\"P-bursts\").alias(\"P-bursts_mean\"),\n        pl.std(\"P-bursts\").alias(\"P-bursts_std\"),\n        pl.count(\"P-bursts\").alias(\"P-bursts_count\"),\n        pl.median(\"P-bursts\").alias(\"P-bursts_median\"),\n        pl.max(\"P-bursts\").alias(\"P-bursts_max\"),\n        pl.first(\"P-bursts\").alias(\"P-bursts_first\"),\n        pl.last(\"P-bursts\").alias(\"P-bursts_last\"),\n    )\n    feats = feats.join(temp, on='id', how='left') \n\n\n    print(\"< R-bursts features >\")\n\n    temp = df.filter(pl.col(\"activity\").is_in(['Input', 'Remove/Cut']))\n    temp = temp.with_columns((pl.col(\"activity\") == 'Remove/Cut').alias(\"is_remove\"))\n    temp = temp.with_columns(pl.col(\"is_remove\").rle_id().alias(\"remove_id\"))\n    temp = temp.with_columns(pl.count().over(\"remove_id\").alias(\"R-bursts\"))\n    temp = temp.filter(pl.col(\"is_remove\"))\n    temp = temp.group_by(\"id\").agg(\n        pl.mean(\"R-bursts\").alias(\"R-bursts_mean\"),\n        pl.std(\"R-bursts\").alias(\"R-bursts_std\"),\n        pl.median(\"R-bursts\").alias(\"R-bursts_median\"),\n        pl.max(\"R-bursts\").alias(\"R-bursts_max\"),\n        pl.first(\"R-bursts\").alias(\"R-bursts_first\"),\n        pl.last(\"R-bursts\").alias(\"R-bursts_last\")\n    )\n    feats = feats.join(temp, on='id', how='left')\n    \n    return feats\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def q1(x):\n    return x.quantile(0.1)\ndef q2(x):\n    return x.quantile(0.25)\ndef q7(x):\n    return x.quantile(0.75)\ndef q9(x):\n    return x.quantile(0.90)\n\nAGGREGATIONS = ['count', 'mean', 'min', 'max', 'first', 'last', q2, 'median', q7,'sum']\n\ndef reconstruct_essay(currTextInput):\n    essayText = \"\"\n    for Input in currTextInput.values:\n        if Input[0] == 'Replace':\n            replaceTxt = Input[2].split(' => ')\n            essayText = essayText[:Input[1] - len(replaceTxt[1])] + replaceTxt[1] + essayText[Input[1] - len(replaceTxt[1]) + len(replaceTxt[0]):]\n            continue\n        if Input[0] == 'Paste':\n            essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n            continue\n        if Input[0] == 'Remove/Cut':\n            essayText = essayText[:Input[1]] + essayText[Input[1] + len(Input[2]):]\n            continue\n        if \"M\" in Input[0]:\n            croppedTxt = Input[0][10:]\n            splitTxt = croppedTxt.split(' To ')\n            valueArr = [item.split(', ') for item in splitTxt]\n            moveData = (int(valueArr[0][0][1:]), int(valueArr[0][1][:-1]), int(valueArr[1][0][1:]), int(valueArr[1][1][:-1]))\n            if moveData[0] != moveData[2]:\n                if moveData[0] < moveData[2]:\n                    essayText = essayText[:moveData[0]] + essayText[moveData[1]:moveData[3]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[3]:]\n                else:\n                    essayText = essayText[:moveData[2]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[2]:moveData[0]] + essayText[moveData[1]:]\n            continue\n        essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n    return essayText\n\n\ndef get_essay_df(df):\n    df       = df[df.activity != 'Nonproduction']\n    temp     = df.groupby('id').apply(lambda x: reconstruct_essay(x[['activity', 'cursor_position', 'text_change']]))\n    essay_df = pd.DataFrame({'id': df['id'].unique().tolist()})\n    essay_df = essay_df.merge(temp.rename('essay'), on='id')\n    return essay_df\n\n\ndef word_feats(df):\n    essay_df = df\n    df['word'] = df['essay'].apply(lambda x: re.split(' |\\\\n|\\\\.|\\\\?|\\\\!',x))\n    df = df.explode('word')\n    df['word_len'] = df['word'].apply(lambda x: len(x))\n    df = df[df['word_len'] != 0]\n\n    word_agg_df = df[['id','word_len']].groupby(['id']).agg(AGGREGATIONS)\n    word_agg_df.columns = ['_'.join(x) for x in word_agg_df.columns]\n    word_agg_df['id'] = word_agg_df.index\n    word_agg_df = word_agg_df.reset_index(drop=True)\n    return word_agg_df\n\n\ndef sent_feats(df):\n    df['sent'] = df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',x))\n    df = df.explode('sent')\n    df['sent'] = df['sent'].apply(lambda x: x.replace('\\n','').strip())\n    # Number of characters in sentences\n    df['sent_len'] = df['sent'].apply(lambda x: len(x))\n    # Number of words in sentences\n    df['sent_word_count'] = df['sent'].apply(lambda x: len(x.split(' ')))\n    df = df[df.sent_len!=0].reset_index(drop=True)\n\n    sent_agg_df = pd.concat([df[['id','sent_len']].groupby(['id']).agg(AGGREGATIONS), \n                             df[['id','sent_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1)\n    sent_agg_df.columns = ['_'.join(x) for x in sent_agg_df.columns]\n    sent_agg_df['id'] = sent_agg_df.index\n    sent_agg_df = sent_agg_df.reset_index(drop=True)\n    sent_agg_df.drop(columns=[\"sent_word_count_count\"], inplace=True)\n    sent_agg_df = sent_agg_df.rename(columns={\"sent_len_count\":\"sent_count\"})\n    return sent_agg_df\n\n\ndef parag_feats(df):\n    df['paragraph'] = df['essay'].apply(lambda x: x.split('\\n'))\n    df = df.explode('paragraph')\n    # Number of characters in paragraphs\n    df['paragraph_len'] = df['paragraph'].apply(lambda x: len(x)) \n    # Number of words in paragraphs\n    df['paragraph_word_count'] = df['paragraph'].apply(lambda x: len(x.split(' ')))\n    df = df[df.paragraph_len!=0].reset_index(drop=True)\n    \n    paragraph_agg_df = pd.concat([df[['id','paragraph_len']].groupby(['id']).agg(AGGREGATIONS), \n                                  df[['id','paragraph_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1) \n    paragraph_agg_df.columns = ['_'.join(x) for x in paragraph_agg_df.columns]\n    paragraph_agg_df['id'] = paragraph_agg_df.index\n    paragraph_agg_df = paragraph_agg_df.reset_index(drop=True)\n    paragraph_agg_df.drop(columns=[\"paragraph_word_count_count\"], inplace=True)\n    paragraph_agg_df = paragraph_agg_df.rename(columns={\"paragraph_len_count\":\"paragraph_count\"})\n    return paragraph_agg_df\n\ndef product_to_keys(logs, essays):\n    essays['product_len'] = essays.essay.str.len()\n    tmp_df = logs[logs.activity.isin(['Input', 'Remove/Cut'])].groupby(['id']).agg({'activity': 'count'}).reset_index().rename(columns={'activity': 'keys_pressed'})\n    essays = essays.merge(tmp_df, on='id', how='left')\n    essays['product_to_keys'] = essays['product_len'] / essays['keys_pressed']\n    return essays[['id', 'product_to_keys']]\n\ndef get_keys_pressed_per_second(logs):\n    temp_df = logs[logs['activity'].isin(['Input', 'Remove/Cut'])].groupby(['id']).agg(keys_pressed=('event_id', 'count')).reset_index()\n    temp_df_2 = logs.groupby(['id']).agg(min_down_time=('down_time', 'min'), max_up_time=('up_time', 'max')).reset_index()\n    temp_df = temp_df.merge(temp_df_2, on='id', how='left')\n    temp_df['keys_per_second'] = temp_df['keys_pressed'] / ((temp_df['max_up_time'] - temp_df['min_down_time']) / 1000)\n    return temp_df[['id', 'keys_per_second']]\n\n\ndef target_encoding(train_df, scores, feature):\n    \n    train_df['target'] = train_df['id'].map(dict(scores.values))\n    \n    down_event_counts = train_df[feature].value_counts()\n    rare_down_events = down_event_counts[down_event_counts <= 3].index\n    # Replace 'target' values with NaN for these rare events\n    train_df.loc[train_df[feature].isin(rare_down_events), 'target'] = np.nan    \n\n    # Step 2: Calculate the mean 'target' for each 'down_event'\n    mean_target_by_down_event = train_df.groupby(feature)['target'].mean().reset_index(name=f'{feature}_mean_target')\n    train_df.drop(columns=[\"target\"], inplace=True)\n    \n    return mean_target_by_down_event","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\nprint('< Read Data >')\ndata_path = '/kaggle/input/linking-writing-processes-to-writing-quality/'\n\n# Train\ntrain_logs = pl.scan_csv(data_path + 'train_logs.csv')\ntrain_feats = dev_feats(train_logs)\ntrain_feats = train_feats.collect().to_pandas()\ntrain_scores = pd.read_csv(data_path + 'train_scores.csv')\ntrain_logs = train_logs.collect().to_pandas().drop([850340],axis=0)\n\n# Test\ntest_logs = pl.scan_csv(data_path + 'test_logs.csv')\ntest_feats = dev_feats(test_logs)\ntest_feats = test_feats.collect().to_pandas()\ntest_logs = test_logs.collect().to_pandas()\n\nprint('< Train Features >')\ntrain_essays = get_essay_df(train_logs)\ntrain_feats = train_feats.merge(word_feats(train_essays), on='id', how='left')\ntrain_feats = train_feats.merge(sent_feats(train_essays), on='id', how='left')\ntrain_feats = train_feats.merge(parag_feats(train_essays), on='id', how='left')\ntrain_feats = train_feats.merge(get_keys_pressed_per_second(train_logs), on='id', how='left')\ndf_train_SB = train_feats.merge(product_to_keys(train_logs, train_essays), on='id', how='left')\n    \nprint('< Test Features >')\ntest_essays = get_essay_df(test_logs)\ntest_feats = test_feats.merge(word_feats(test_essays), on='id', how='left')\ntest_feats = test_feats.merge(sent_feats(test_essays), on='id', how='left')\ntest_feats = test_feats.merge(parag_feats(test_essays), on='id', how='left')\ntest_feats = test_feats.merge(get_keys_pressed_per_second(test_logs), on='id', how='left')\ndf_test_SB = test_feats.merge(product_to_keys(test_logs, test_essays), on='id', how='left')\n\nprint(\"SB Process Complete\")\n\ndisplay(df_train_SB)\ndisplay(df_test_SB)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re, math, numpy as np, pandas as pd\n\ndef enrich_full_text_features_parallel(df, show_preview=True):\n    \"\"\"\n    Parallelized full essay feature extractor (~84 features total).\n    Combines every linguistic, structural, and punctuation-based feature\n    from your raw pipeline into a single efficient parallelized pass.\n\n    Requires: swifter (optional, auto-fallback if not installed)\n    \"\"\"\n\n    # ---------- Safe import of swifter ----------\n    try:\n        import swifter\n        use_swifter = True\n        print(\"‚ö° Using swifter for parallel processing\")\n    except ImportError:\n        use_swifter = False\n        print(\"‚ÑπÔ∏è swifter not installed ‚Äî using normal .apply() (slower)\")\n\n    df = df.copy()\n\n    # ---------- Inner per-essay feature extractor ----------\n    def _extract_features(text: str):\n        text = str(text)\n        sentences = [s.strip() for s in re.split(r\"[.!?]+\", text) if s.strip()]\n        n_sent = len(sentences)\n        paragraphs = [p.strip() for p in re.split(r\"(?:\\r?\\n\\s*\\r?\\n)+\", text.strip()) if p.strip()]\n        n_par = len(paragraphs)\n        words = re.findall(r\"\\b[a-zA-Z]+\\b\", text)\n        n_words = len(words)\n\n        per100_tokens = lambda n: (n / n_words * 100.0) if n_words > 0 else 0.0\n        per100_sents  = lambda n: (n / n_sent * 100.0) if n_sent > 0 else 0.0\n        tok_count = lambda s: len(re.findall(r\"\\b[a-zA-Z]+\\b\", s))\n        internal_punct_count = lambda s: s.count(\",\") + s.count(\";\") + s.count(\":\")\n\n        # ---------- Basic counts ----------\n        num_words, num_sentences, num_paragraphs = n_words, n_sent, n_par\n\n        if n_sent:\n            lengths = [tok_count(s) for s in sentences]\n            mean_sentence_len = np.mean(lengths)\n            std_sentence_len  = np.std(lengths)\n            cv_sentence_len   = std_sentence_len / mean_sentence_len if mean_sentence_len > 0 else 0\n            short_sent_share  = np.mean(np.array(lengths) <= 5)\n            long_sent_share   = np.mean(np.array(lengths) >= 20)\n        else:\n            mean_sentence_len = std_sentence_len = cv_sentence_len = short_sent_share = long_sent_share = 0.0\n\n        # ---------- Paragraph structure ----------\n        if n_par:\n            sent_per_para = [len([s for s in re.split(r\"[.!?]+\", p) if s.strip()]) for p in paragraphs]\n            word_counts = [tok_count(p) for p in paragraphs]\n            avg_sent_per_para = np.mean(sent_per_para)\n            var_sent_per_para = np.var(sent_per_para)\n            intro_para_len = word_counts[0]\n            body_para_mean_len = np.mean(word_counts[1:-1]) if n_par > 2 else 0\n            conclusion_para_len = word_counts[-1] if n_par > 1 else 0\n        else:\n            avg_sent_per_para = var_sent_per_para = intro_para_len = body_para_mean_len = conclusion_para_len = 0.0\n\n        # ---------- Comma density ----------\n        num_commas = text.count(\",\")\n        commas_per_sentence = num_commas / n_sent if n_sent else 0\n        commas_per_100_words = per100_tokens(num_commas)\n        multi_clause_sent_share = np.mean([s.count(\",\") >= 2 for s in sentences]) if n_sent else 0\n\n        # ---------- Semicolon / colon ----------\n        num_semis, num_colons = text.count(\";\"), text.count(\":\")\n        semicolons_per_100_tokens = per100_tokens(num_semis)\n        colons_per_100_tokens = per100_tokens(num_colons)\n        share_sents_with_semicolon = (sum(\";\" in s for s in sentences) / n_sent) if n_sent else 0\n        share_sents_with_colon = (sum(\":\" in s for s in sentences) / n_sent) if n_sent else 0\n\n        # ---------- Parentheses / quotes / dashes ----------\n        SINGLE_QUOTES = [\"'\", \"‚Äò\", \"‚Äô\", \"‚Äö\", \"‚Äõ\"]\n        DOUBLE_QUOTES = ['\"', \"‚Äú\", \"‚Äù\", \"‚Äû\", \"‚Äü\"]\n        DASHES = [\"-\", \"‚Äì\", \"‚Äî\"]\n        left_paren, right_paren = text.count(\"(\"), text.count(\")\")\n        parentheses = left_paren + right_paren\n        single_q = sum(text.count(ch) for ch in SINGLE_QUOTES)\n        double_q = sum(text.count(ch) for ch in DOUBLE_QUOTES)\n        dashes = sum(text.count(ch) for ch in DASHES)\n        counts = [parentheses, single_q, double_q, dashes]\n        total = sum(counts)\n        if total:\n            p = [c / total for c in counts if c > 0]\n            H = -sum(pi * math.log(pi, 2) for pi in p)\n            H_norm = H / math.log(4, 2)\n        else:\n            H = H_norm = 0.0\n\n        # ---------- Mechanics consistency ----------\n        unmatched_parens_open = max(0, left_paren - right_paren)\n        unmatched_parens_close = max(0, right_paren - left_paren)\n        mismatched_parens_total = unmatched_parens_open + unmatched_parens_close\n\n        text_no_apos = re.sub(r\"(?<=\\w)[\\'‚Äô](?=\\w)\", \"\", text)\n        straight_single = text_no_apos.count(\"'\")\n        straight_double = text_no_apos.count('\"')\n        unmatched_straight_single = straight_single % 2\n        unmatched_straight_double = straight_double % 2\n        left_single = text_no_apos.count(\"‚Äò\")\n        right_single = text_no_apos.count(\"‚Äô\")\n        left_double = text_no_apos.count(\"‚Äú\")\n        right_double = text_no_apos.count(\"‚Äù\")\n        mismatched_curly_single = abs(left_single - right_single)\n        mismatched_curly_double = abs(left_double - right_double)\n        mismatched_quotes_total = unmatched_straight_single + unmatched_straight_double + mismatched_curly_single + mismatched_curly_double\n\n        def count_repeats(ch): return len(re.findall(re.escape(ch) + r\"{2,}\", text))\n        repeated_commas = count_repeats(\",\")\n        repeated_periods = len(re.findall(r\"\\.{2,}\", text))\n        repeated_semis = count_repeats(\";\")\n        repeated_colons = count_repeats(\":\")\n        repeated_qmarks = count_repeats(r\"\\?\")\n        repeated_exclaims = count_repeats(\"!\")\n        repeated_dashes = sum(count_repeats(ch) for ch in DASHES)\n        repeated_punct_sequences_total = (\n            repeated_commas + repeated_periods + repeated_semis + repeated_colons +\n            repeated_qmarks + repeated_exclaims + repeated_dashes\n        )\n        repeated_punct_sequences_per_100_tokens = per100_tokens(repeated_punct_sequences_total)\n        spaces_before_comma = len(re.findall(r\"\\s+,\", text))\n        spaces_before_punct_total = len(re.findall(r\"\\s+[,\\.;:\\?\\!)]\", text))\n        spaces_before_punct_per_100_tokens = per100_tokens(spaces_before_punct_total)\n        double_spaces_after_eos = len(re.findall(r\"[.!?]\\s{2,}\", text))\n        double_spaces_after_eos_per_100_sentences = per100_sents(double_spaces_after_eos)\n\n        # ---------- Multi-clause proxy ----------\n        if n_sent:\n            counts_int = [internal_punct_count(s) for s in sentences]\n            multi_clause_proxy_share = np.mean(np.array(counts_int) >= 2)\n            any_internal_punct_share = np.mean(np.array(counts_int) >= 1)\n            avg_internal_punct_per_sentence = np.mean(counts_int)\n        else:\n            multi_clause_proxy_share = any_internal_punct_share = avg_internal_punct_per_sentence = 0.0\n\n        # ---------- Rhythm variety ----------\n        if n_sent:\n            sent_lengths = np.array([tok_count(s) for s in sentences], dtype=float)\n            mean_len = sent_lengths.mean()\n            std_len  = sent_lengths.std(ddof=0)\n            cv_global = std_len / mean_len if mean_len > 0 else 0\n            WINDOW = 5\n            if n_sent < WINDOW:\n                cvs = [cv_global]\n            else:\n                cvs = [(sent_lengths[i:i+WINDOW].std(ddof=0) /\n                        sent_lengths[i:i+WINDOW].mean()) if sent_lengths[i:i+WINDOW].mean() > 0 else 0\n                        for i in range(n_sent - WINDOW + 1)]\n            cvs = np.array(cvs)\n            cv_mw_mean   = cvs.mean()   if cvs.size else 0\n            cv_mw_median = np.median(cvs) if cvs.size else 0\n            cv_mw_max    = cvs.max()    if cvs.size else 0\n            cv_mw_std    = cvs.std(ddof=0) if cvs.size else 0\n        else:\n            mean_len = std_len = cv_global = cv_mw_mean = cv_mw_median = cv_mw_max = cv_mw_std = 0.0\n\n        # ---------- Local continuity / segmentation ----------\n        para_sents = [[s.strip() for s in re.split(r\"[.!?]+\", p) if s.strip()] for p in paragraphs]\n        if n_par == 0:\n            single_sentence_paragraph_ratio = bridge_sentence_share = bridge_sentences_per_100_sentences = \\\n            heavy_internal_punct_sentence_share = heavy_at_paragraph_edges_share = heavy_sentence_mean_normalized_position = \\\n            semicolon_sentence_share = semicolon_at_paragraph_edges_share = colon_sentence_share = colon_at_paragraph_edges_share = 0.0\n        else:\n            single_sentence_paragraph_ratio = sum(len(ps) == 1 for ps in para_sents) / n_par\n            all_sents = [s for ps in para_sents for s in ps]\n            n_sent_total = len(all_sents)\n            sent_lengths_all = [tok_count(s) for s in all_sents]\n            bridge_flags = np.array(sent_lengths_all) <= 5\n            bridge_sentence_share = bridge_flags.mean() if bridge_flags.size else 0\n            bridge_sentences_per_100_sentences = bridge_sentence_share * 100\n            heavy_flags = [internal_punct_count(s) >= 2 or \";\" in s or \":\" in s for s in all_sents]\n            heavy_internal_punct_sentence_share = np.mean(heavy_flags) if n_sent_total else 0\n            sent_meta = [(p_idx, i, len(ps)) for p_idx, ps in enumerate(para_sents) for i, _ in enumerate(ps)]\n            heavy_idx = [i for i, h in enumerate(heavy_flags) if h]\n            heavy_edges = sum(1 for gi in heavy_idx if sent_meta[gi][1] in (0, sent_meta[gi][2]-1))\n            heavy_at_paragraph_edges_share = heavy_edges / len(heavy_idx) if heavy_idx else 0\n            heavy_sentence_mean_normalized_position = np.mean([i/(n_sent_total-1) for i,h in enumerate(heavy_flags) if h]) if n_sent_total>1 else 0\n            semi_flags = [\";\" in s for s in all_sents]\n            colon_flags = [\":\" in s for s in all_sents]\n            def edge_share(mask):\n                idxs = [i for i,f in enumerate(mask) if f]\n                return sum(1 for gi in idxs if sent_meta[gi][1] in (0, sent_meta[gi][2]-1)) / len(idxs) if idxs else 0\n            semicolon_sentence_share = np.mean(semi_flags) if n_sent_total else 0\n            semicolon_at_paragraph_edges_share = edge_share(semi_flags)\n            colon_sentence_share = np.mean(colon_flags) if n_sent_total else 0\n            colon_at_paragraph_edges_share = edge_share(colon_flags)\n\n        # ---------- List / explanation patterns ----------\n        colon_sents = [s for s in sentences if \":\" in s]\n        n_colon = len(colon_sents)\n        if n_sent == 0:\n            colon_sentence_share2 = list_like_all = list_like_among = semi_tail_share = avg_trailing = \\\n            items_mean = items_median = items_max = items_ge3 = 0.0\n        else:\n            colon_sentence_share2 = n_colon / n_sent\n            list_like_flags, semi_tail_flags, trailing_counts, items_counts = [], [], [], []\n            for s in colon_sents:\n                _, tail = s.split(\":\", 1)\n                commas, semis = tail.count(\",\"), tail.count(\";\")\n                total_internal = commas + semis\n                trailing_counts.append(total_internal)\n                semi_tail_flags.append(semis > 0)\n                list_like_flags.append(total_internal >= 2)\n                segments = [seg.strip() for seg in re.split(r\"[;,]\", tail)]\n                items = [seg for seg in segments if re.search(r\"\\b[a-zA-Z]+\\b\", seg)]\n                items_counts.append(len(items))\n            list_like_all   = np.mean(list_like_flags) if n_sent else 0\n            list_like_among = np.mean(list_like_flags) if n_colon else 0\n            semi_tail_share = np.mean(semi_tail_flags) if n_colon else 0\n            avg_trailing = np.mean(trailing_counts) if trailing_counts else 0\n            items_mean   = np.mean(items_counts) if items_counts else 0\n            items_median = np.median(items_counts) if items_counts else 0\n            items_max    = np.max(items_counts) if items_counts else 0\n            items_ge3    = np.mean(np.array(items_counts) >= 3) if items_counts else 0\n\n        return pd.Series([\n            num_words,num_sentences,num_paragraphs,\n            mean_sentence_len,std_sentence_len,cv_sentence_len,short_sent_share,long_sent_share,\n            avg_sent_per_para,var_sent_per_para,intro_para_len,body_para_mean_len,conclusion_para_len,\n            commas_per_sentence,commas_per_100_words,multi_clause_sent_share,\n            semicolons_per_100_tokens,colons_per_100_tokens,share_sents_with_semicolon,share_sents_with_colon,\n            parentheses,left_paren,right_paren,single_q,double_q,dashes,H,H_norm,\n            unmatched_parens_open,unmatched_parens_close,mismatched_parens_total,\n            unmatched_straight_single,unmatched_straight_double,mismatched_curly_single,mismatched_curly_double,mismatched_quotes_total,\n            repeated_commas,repeated_periods,repeated_semis,repeated_colons,repeated_qmarks,repeated_exclaims,repeated_dashes,\n            repeated_punct_sequences_total,repeated_punct_sequences_per_100_tokens,\n            spaces_before_comma,spaces_before_punct_total,spaces_before_punct_per_100_tokens,\n            double_spaces_after_eos,double_spaces_after_eos_per_100_sentences,\n            multi_clause_proxy_share,any_internal_punct_share,avg_internal_punct_per_sentence,\n            mean_len,std_len,cv_global,cv_mw_mean,cv_mw_median,cv_mw_max,cv_mw_std,\n            num_paragraphs,single_sentence_paragraph_ratio,bridge_sentence_share,bridge_sentences_per_100_sentences,\n            heavy_internal_punct_sentence_share,heavy_at_paragraph_edges_share,heavy_sentence_mean_normalized_position,\n            semicolon_sentence_share,semicolon_at_paragraph_edges_share,colon_sentence_share,colon_at_paragraph_edges_share,\n            colon_sentence_share2,list_like_all,list_like_among,semi_tail_share,avg_trailing,\n            items_mean,items_median,items_max,items_ge3\n        ])\n\n    # ---------- Column names ----------\n    cols = [\n        'num_words','num_sentences','num_paragraphs',\n        'mean_sentence_len','std_sentence_len','cv_sentence_len','short_sent_share','long_sent_share',\n        'avg_sent_per_para','var_sent_per_para','intro_para_len','body_para_mean_len','conclusion_para_len',\n        'commas_per_sentence','commas_per_100_words','multi_clause_sent_share',\n        'semicolons_per_100_tokens','colons_per_100_tokens','share_sents_with_semicolon','share_sents_with_colon',\n        'parentheses_count','left_parentheses_count','right_parentheses_count','single_quotes_count','double_quotes_count','dashes_count','punct_diversity_shannon','punct_diversity_shannon_norm',\n        'unmatched_parens_open','unmatched_parens_close','mismatched_parens_total',\n        'unmatched_quotes_straight_single','unmatched_quotes_straight_double','mismatched_quotes_curly_single','mismatched_quotes_curly_double','mismatched_quotes_total',\n        'repeated_commas_seq','repeated_periods_seq','repeated_semicolons_seq','repeated_colons_seq','repeated_qmarks_seq','repeated_exclaims_seq','repeated_dashes_seq',\n        'repeated_punct_sequences_total','repeated_punct_sequences_per_100_tokens',\n        'spaces_before_comma','spaces_before_punct_total','spaces_before_punct_per_100_tokens',\n        'double_spaces_after_eos','double_spaces_after_eos_per_100_sentences',\n        'multi_clause_proxy_share','any_internal_punct_share','avg_internal_punct_per_sentence',\n        'sent_len_tokens_mean','sent_len_tokens_std','sent_len_tokens_cv_global',\n        'sent_len_tokens_cv_mw_mean','sent_len_tokens_cv_mw_median','sent_len_tokens_cv_mw_max','sent_len_tokens_cv_mw_std',\n        'num_paragraphs','single_sentence_paragraph_ratio','bridge_sentence_share','bridge_sentences_per_100_sentences',\n        'heavy_internal_punct_sentence_share','heavy_at_paragraph_edges_share','heavy_sentence_mean_normalized_position',\n        'semicolon_sentence_share','semicolon_at_paragraph_edges_share','colon_sentence_share','colon_at_paragraph_edges_share',\n        'colon_sentence_share_2','list_like_colon_sentence_share_all','list_like_colon_sentence_share_among_colon',\n        'semicolon_in_tail_share_among_colon','avg_trailing_commas_semis_per_colon_sent',\n        'items_after_colon_mean','items_after_colon_median','items_after_colon_max','items_ge3_share_among_colon'\n    ]\n\n    # ---------- Parallel apply ----------\n    if use_swifter:\n        feature_df = df[\"essay_text\"].swifter.progress_bar(True).apply(_extract_features)\n    else:\n        feature_df = df[\"essay_text\"].apply(_extract_features)\n\n    feature_df.columns = cols\n    df_out = pd.concat([df.reset_index(drop=True), feature_df], axis=1)\n\n    if show_preview:\n        print(f\"‚úÖ Feature enrichment complete: {len(cols)} new columns added for {len(df_out)} essays.\")\n        display(df_out.head(2)[['num_words','mean_sentence_len','multi_clause_proxy_share','items_after_colon_mean']])\n\n    return df_out","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Derivation of Features From Research (Function)","metadata":{}},{"cell_type":"code","source":"# ==========================================================\n# üöÄ FULL ESSAY RECONSTRUCTION + FEATURE ENRICHMENT PIPELINE\n# ==========================================================\n\n# ‚úÖ Step 0: Start from original logs\ndf_train_logs_copy = df_train_logs.copy()\nprint(\"üìò Step 0: Original df_train_logs shape:\", df_train_logs_copy.shape)\n\n# ‚úÖ Step 1: Essay reconstruction\ndf_train_recon_logs_raw = getEssays(df_train_logs_copy.copy()).to_frame(name='essay_text')\ndf_train_recon_logs_raw.index.name = 'id'\ndf_train_recon_logs_raw.reset_index(inplace=True)  # ensure 'id' is a proper column\nprint(\"‚úÖ Step 1: Essays reconstructed ‚Äî shape:\", df_train_recon_logs_raw.shape)\n\n# ==========================================================\n# ‚úÖ Step 2: Unified feature enrichment (parallelized)\n# ==========================================================\nprint(\"‚öôÔ∏è Step 2: Extracting full linguistic + structural + mechanics features (parallelized)...\")\ndf_train_recon_logs = enrich_full_text_features_parallel(df_train_recon_logs_raw.copy(), show_preview=False)\nprint(\"‚úÖ Step 2: Feature enrichment complete ‚Äî shape:\", df_train_recon_logs.shape)\n\n# ==========================================================\n# ‚úÖ Step 3: Check for duplicate columns\n# ==========================================================\ndupes = df_train_recon_logs.columns[df_train_recon_logs.columns.duplicated()]\n\nif len(dupes) > 0:\n    from collections import Counter\n    dupe_counts = Counter(dupes)\n    print(f\"\\n‚ö†Ô∏è Found {len(dupe_counts)} duplicate column names:\")\n    for name, count in list(dupe_counts.items())[:15]:\n        print(f\"   üß© {name} ‚Üí appears {count} times\")\n    if len(dupe_counts) > 15:\n        print(\"   ... (truncated)\")\n    \n    # Drop duplicates (keep first occurrence)\n    before = df_train_recon_logs.shape[1]\n    df_train_recon_logs = df_train_recon_logs.loc[:, ~df_train_recon_logs.columns.duplicated()]\n    after = df_train_recon_logs.shape[1]\n    print(f\"üßπ Removed {before - after} duplicate columns. Final shape: {df_train_recon_logs.shape}\")\nelse:\n    print(\"\\n‚úÖ No duplicate columns detected in df_train_recon_logs.\")\n\n# ==========================================================\n# ‚úÖ SUMMARY\n# ==========================================================\nprint(\"\\nüéØ Pipeline complete! Final dataset ‚Üí df_train_recon_logs\")\nprint(f\"üß© Step 0: df_train_logs_copy shape: {df_train_logs_copy.shape}\")\nprint(f\"üß© Step 1: df_train_recon_logs_raw shape: {df_train_recon_logs_raw.shape}\")\nprint(f\"üß© Step 2: df_train_recon_logs (final) shape: {df_train_recon_logs.shape}\")\n\n# üß† Sanity check\nprint(\"üß† Total essays:\", df_train_recon_logs.shape[0])\nprint(\"üß© Total new features:\", df_train_recon_logs.shape[1] - 2)  # exclude id + essay_text\n\n# ‚úÖ Optional preview\ndisplay(df_train_recon_logs.head(2)[['id', 'essay_text'] + df_train_recon_logs.columns[2:12].tolist()])\n\n# (Optional) Save for reuse\n# df_train_recon_logs.to_csv(\"/kaggle/working/df_train_recon_logs.csv\", index=False)\n# print(\"üíæ Saved df_train_recon_logs.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================================================\n# üöÄ FULL ESSAY RECONSTRUCTION + FEATURE ENRICHMENT PIPELINE (Test)\n# ==========================================================\n\n# ‚úÖ Step 0: Start from original logs\ndf_test_logs_copy = df_test_logs.copy()\nprint(\"üìò Step 0: Original df_test_logs shape:\", df_test_logs_copy.shape)\n\n# ‚úÖ Step 1: Essay reconstruction\ndf_test_recon_logs_raw = getEssays(df_test_logs_copy.copy()).to_frame(name='essay_text')\ndf_test_recon_logs_raw.index.name = 'id'\ndf_test_recon_logs_raw.reset_index(inplace=True)  # ensure 'id' is a proper column\nprint(\"‚úÖ Step 1: Essays reconstructed ‚Äî shape:\", df_test_recon_logs_raw.shape)\n\n# ==========================================================\n# ‚úÖ Step 2: Unified feature enrichment (parallelized)\n# ==========================================================\nprint(\"‚öôÔ∏è Step 2: Extracting full linguistic + structural + mechanics features (parallelized)...\")\ndf_test_recon_logs = enrich_full_text_features_parallel(df_test_recon_logs_raw.copy(), show_preview=False)\nprint(\"‚úÖ Step 2: Feature enrichment complete ‚Äî shape:\", df_test_recon_logs.shape)\n\n# ==========================================================\n# ‚úÖ Step 3: Check for duplicate columns\n# ==========================================================\ndupes = df_test_recon_logs.columns[df_test_recon_logs.columns.duplicated()]\n\nif len(dupes) > 0:\n    from collections import Counter\n    dupe_counts = Counter(dupes)\n    print(f\"\\n‚ö†Ô∏è Found {len(dupe_counts)} duplicate column names:\")\n    for name, count in list(dupe_counts.items())[:15]:\n        print(f\"   üß© {name} ‚Üí appears {count} times\")\n    if len(dupe_counts) > 15:\n        print(\"   ... (truncated)\")\n    \n    # Drop duplicates (keep first occurrence)\n    before = df_test_recon_logs.shape[1]\n    df_test_recon_logs = df_test_recon_logs.loc[:, ~df_test_recon_logs.columns.duplicated()]\n    after = df_test_recon_logs.shape[1]\n    print(f\"üßπ Removed {before - after} duplicate columns. Final shape: {df_test_recon_logs.shape}\")\nelse:\n    print(\"\\n‚úÖ No duplicate columns detected in df_test_recon_logs.\")\n\n# ==========================================================\n# ‚úÖ SUMMARY\n# ==========================================================\nprint(\"\\nüéØ Pipeline complete! Final dataset ‚Üí df_test_recon_logs\")\nprint(f\"üß© Step 0: df_test_logs_copy shape: {df_test_logs_copy.shape}\")\nprint(f\"üß© Step 1: df_test_recon_logs_raw shape: {df_test_recon_logs_raw.shape}\")\nprint(f\"üß© Step 2: df_test_recon_logs (final) shape: {df_test_recon_logs.shape}\")\n\n# üß† Sanity check\nprint(\"üß† Total essays:\", df_test_recon_logs.shape[0])\nprint(\"üß© Total new features:\", df_test_recon_logs.shape[1] - 2)  # exclude id + essay_text\n\n# ‚úÖ Optional preview\ndisplay(df_test_recon_logs.head(2)[['id', 'essay_text'] + df_test_recon_logs.columns[2:12].tolist()])\n\n# (Optional) Save for reuse\n# df_test_recon_logs.to_csv(\"/kaggle/working/df_test_recon_logs.csv\", index=False)\n# print(\"üíæ Saved df_test_recon_logs.csv\")","metadata":{"trusted":true,"_kg_hide-input":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Apply Raw Tokenization","metadata":{}},{"cell_type":"code","source":"def reconstruct_essay(currTextInput):\n    essayText = \"\"\n    for Input in currTextInput.values:\n        if Input[0] == 'Replace':\n            replaceTxt = Input[2].split(' => ')\n            essayText = essayText[:Input[1] - len(replaceTxt[1])] + replaceTxt[1] + essayText[Input[1] - len(replaceTxt[1]) + len(replaceTxt[0]):]\n            continue\n        if Input[0] == 'Paste':\n            essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n            continue\n        if Input[0] == 'Remove/Cut':\n            essayText = essayText[:Input[1]] + essayText[Input[1] + len(Input[2]):]\n            continue\n        if \"M\" in Input[0]:\n            croppedTxt = Input[0][10:]\n            splitTxt = croppedTxt.split(' To ')\n            valueArr = [item.split(', ') for item in splitTxt]\n            moveData = (int(valueArr[0][0][1:]), int(valueArr[0][1][:-1]), int(valueArr[1][0][1:]), int(valueArr[1][1][:-1]))\n            if moveData[0] != moveData[2]:\n                if moveData[0] < moveData[2]:\n                    essayText = essayText[:moveData[0]] + essayText[moveData[1]:moveData[3]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[3]:]\n                else:\n                    essayText = essayText[:moveData[2]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[2]:moveData[0]] + essayText[moveData[1]:]\n            continue\n        essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n    return essayText\n\ndef get_essay_df(df):\n    # Filter out 'Nonproduction' activities, group by 'id', and apply 'reconstruct_essay' function\n    df_essay = df[df.activity != 'Nonproduction'].groupby('id').apply(lambda x: reconstruct_essay(x[['activity', 'cursor_position', 'text_change']]))\n    \n    # Reset the index and rename the column to 'essay'\n    df_essay = df_essay.reset_index(name='essay')\n    \n    return df_essay\n\n## NOTE: Simplified logic for get_essay_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\ndf_train_essays = get_essay_df(df_train_logs)\ndf_test_essays = get_essay_df(df_test_logs)\n\ndisplay(df_train_essays)\ndisplay(df_test_essays)\n\n# id and essay","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\ndf_train_to_tokenise = df_train_essays[['id', 'essay']].copy()\ndf_test_to_tokenise = df_test_essays[['id', 'essay']].copy()\n\n# Step 1: Tokenize the text using CountVectorizer\ncount_vectorizer = CountVectorizer(ngram_range=(1, 4), analyzer='char_wb')\n\n# Transform essays into frequency vectors\nX_tokenizer_train = count_vectorizer.fit_transform(df_train_to_tokenise['essay']).todense()\nX_tokenizer_test = count_vectorizer.transform(df_test_to_tokenise['essay']).todense()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: Convert the sparse matrix to DataFrame\ndf_train_tokenised = pd.DataFrame(X_tokenizer_train, columns=count_vectorizer.get_feature_names_out())\ndf_test_tokenised = pd.DataFrame(X_tokenizer_test, columns=count_vectorizer.get_feature_names_out())\n\n# Step 2: Rename columns to \"feature 0\", \"feature 1\", \"feature 2\", etc.\ndf_train_tokenised.columns = [f\"feature {i}\" for i in range(df_train_tokenised.shape[1])]\ndf_test_tokenised.columns = [f\"feature {i}\" for i in range(df_test_tokenised.shape[1])]\n\n# Step 3: Add the 'id' column to the training and test DataFrames at the start\ndf_train_tokenised.insert(0, 'id', df_train_essays['id'])\ndf_test_tokenised.insert(0, 'id', df_test_essays['id'])\n\n# Display the results\ndisplay(df_train_tokenised)\ndisplay(df_test_tokenised)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Apply Linear Discriminant Analysis (LDA)","metadata":{}},{"cell_type":"code","source":"df_train_LDA = df_train_essays[['id']].copy()\ndf_test_LDA = df_test_essays[['id']].copy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"display(df_train_LDA)\ndisplay(df_test_LDA)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nCouVec = CountVectorizer(stop_words='english')\ndf = pd.concat([df_train_essays,df_test_essays])\nCouVec.fit(df['essay'])\ntrain_words = pd.DataFrame(CouVec.transform(df_train_essays['essay']).toarray())\ntest_words = pd.DataFrame(CouVec.transform(df_test_essays['essay']).toarray())\n\nfrom sklearn.decomposition import LatentDirichletAllocation\nn_clusters = 6\nLDA = LatentDirichletAllocation(n_components=n_clusters, max_iter=10, random_state=42, verbose=True)\nLDA.fit(pd.concat([train_words,test_words]))\nTopics = [f'Topic_{x}' for x in range(0,n_clusters)]\ndf_train_LDA[Topics] = LDA.transform(train_words)\ndf_test_LDA[Topics] = LDA.transform(test_words)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nCouVec = CountVectorizer(analyzer='char_wb')\ndf = pd.concat([df_train_essays,df_test_essays])\nCouVec.fit(df['essay'])\ntrain_words = pd.DataFrame(CouVec.transform(df_train_essays['essay']).toarray())\ntest_words = pd.DataFrame(CouVec.transform(df_test_essays['essay']).toarray())\n\nfrom sklearn.decomposition import LatentDirichletAllocation\nn_clusters = 6\nLDA = LatentDirichletAllocation(n_components=n_clusters, max_iter=10, random_state=42, verbose=True)\nLDA.fit(pd.concat([train_words,test_words]))\nTopics = [f'_Topic_{x}' for x in range(0,n_clusters)]\ndf_train_LDA[Topics] = LDA.transform(train_words)\ndf_test_LDA[Topics] = LDA.transform(test_words)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nCouVec = CountVectorizer(analyzer='char_wb',ngram_range=(5, 6))\ndf = pd.concat([df_train_essays,df_test_essays])\nCouVec.fit(df['essay'])\ntrain_words = pd.DataFrame(CouVec.transform(df_train_essays['essay']).toarray())\ntest_words = pd.DataFrame(CouVec.transform(df_test_essays['essay']).toarray())\n\nfrom sklearn.decomposition import LatentDirichletAllocation\nn_clusters = 6\nLDA = LatentDirichletAllocation(n_components=n_clusters, max_iter=10, random_state=42, verbose=True)\nLDA.fit(pd.concat([train_words,test_words]))\nTopics = [f'__Topic_{x}' for x in range(0,n_clusters)]\ndf_train_LDA[Topics] = LDA.transform(train_words)\ndf_test_LDA[Topics] = LDA.transform(test_words)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Obtain DeBERTa Embeddings (Function)","metadata":{}},{"cell_type":"code","source":"import os, torch, numpy as np, pandas as pd\nfrom transformers import AutoTokenizer, AutoModel\nfrom math import ceil\nfrom pathlib import Path\n\ndef add_deberta_embeddings(\n    df,\n    model_dir=\"/kaggle/input/deberta-v3-fast-tokenizer-copy/deb-v3\",\n    text_col=\"essay_text\",\n    id_col=\"id\",\n    max_len=256,\n    batch_size=8,\n    show_preview=True\n):\n    \"\"\"\n    Adds mean-pooled DeBERTa-v3 embeddings as new columns (deb_emb_0 ... deb_emb_767)\n    to a DataFrame containing essays.\n\n    Internal behavior:\n      - Replaces all standalone 'q' tokens with 'i' *only for embedding computation*\n      - Original text in df is NOT modified or returned altered\n    \"\"\"\n\n    assert {id_col, text_col}.issubset(df.columns), f\"Missing {id_col} or {text_col}\"\n    df = df.copy()  # prevent in-place mutation\n\n    # =====================\n    # ‚öôÔ∏è Setup\n    # =====================\n    os.environ[\"HF_HUB_OFFLINE\"] = \"1\"\n    os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n\n    print(f\"\\nüîß Loading DeBERTa model from: {model_dir}\")\n    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(\"üíª Device:\", DEVICE)\n\n    tok = AutoTokenizer.from_pretrained(model_dir, local_files_only=True)\n    model = AutoModel.from_pretrained(model_dir, local_files_only=True).to(DEVICE).eval()\n\n    # =====================\n    # üß† Internal helper: q ‚Üí i replacement\n    # =====================\n    @torch.inference_mode()\n    def preprocess_texts(texts):\n        \"\"\"\n        Replace lowercase standalone 'q' with 'i' before embedding.\n        This change is temporary and not persisted to the DataFrame.\n        \"\"\"\n        return [pd.Series(t).astype(str).str.replace(r'\\bq\\b', 'i', regex=True).iloc[0].strip() for t in texts]\n\n    @torch.inference_mode()\n    def masked_mean_pool(last_hidden_state, mask):\n        mask = mask.unsqueeze(-1)\n        summed = (last_hidden_state * mask).sum(1)\n        count = mask.sum(1).clamp(min=1e-9)\n        return summed / count\n\n    @torch.inference_mode()\n    def embed_texts(texts):\n        all_embs = []\n        n = len(texts)\n        for b in range(ceil(n / batch_size)):\n            batch = texts[b * batch_size:(b + 1) * batch_size]\n            enc = tok(batch, padding=True, truncation=True, max_length=max_len, return_tensors=\"pt\").to(DEVICE)\n            out = model(**enc)\n            pooled = masked_mean_pool(out.last_hidden_state, enc[\"attention_mask\"])\n            all_embs.append(pooled.cpu().numpy())\n        return np.vstack(all_embs).astype(\"float32\")\n\n    # =====================\n    # üß© Embed essays (with temporary cleaned text)\n    # =====================\n    texts_original = df[text_col].astype(str).tolist()\n    texts_cleaned  = preprocess_texts(texts_original)  # temporary replacement\n    ids = df[id_col].values\n\n    print(f\"\\nüìù Embedding {len(texts_cleaned)} essays | max_len={max_len}, batch_size={batch_size}\")\n\n    X_emb = embed_texts(texts_cleaned)\n    emb_df = pd.DataFrame(X_emb, columns=[f\"deb_emb_{i}\" for i in range(X_emb.shape[1])])\n    emb_df[id_col] = ids\n\n    df_out = df.merge(emb_df, on=id_col, how=\"left\")\n\n    # =====================\n    # ‚úÖ Verification output\n    # =====================\n    print(f\"‚úÖ Done! Added {X_emb.shape[1]} embedding columns.\")\n    print(f\"üìä Output shape: {df_out.shape}\")\n    if show_preview:\n        print(\"\\nüîç Preview of first 2 rows and first 5 embedding dims:\")\n        display(df_out[[id_col, text_col] + [f\"deb_emb_{i}\" for i in range(5)]].head(2))\n\n    return df_out","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================================================\n#  STEP: Add DeBERTa Embeddings to Reconstructed Essays\n# ==========================================================\n\nprint(\"‚öôÔ∏è  Applying DeBERTa embeddings to training essays...\")\n\n# Apply to df_train_recon_logs\ndf_train_recon_D_logs = add_deberta_embeddings(df_train_recon_logs.copy())\n\n# ‚úÖ Verification\nprint(\"\\n‚úÖ Embedding process complete!\")\nprint(\"üìä Final DataFrame shape:\", df_train_recon_D_logs.shape)\nprint(\"üß© Sample of new columns added:\")\nprint([col for col in df_train_recon_D_logs.columns if col.startswith(\"deb_emb_\")][:10])\n\n# Optional ‚Äî sanity check for alignment\nid_check = df_train_recon_D_logs[\"id\"].equals(df_train_recon_logs[\"id\"])\nprint(f\"üîç ID alignment check passed? {id_check}\")\n\n# Preview\ndisplay(df_train_recon_D_logs.head(2))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================================================\n#  STEP: Add DeBERTa Embeddings to Reconstructed Essays (Test)\n# ==========================================================\n\nprint(\"‚öôÔ∏è  Applying DeBERTa embeddings to test essays...\")\n\n# Apply to df_test_recon_logs\ndf_test_recon_D_logs = add_deberta_embeddings(df_test_recon_logs.copy())\n\n# ‚úÖ Verification\nprint(\"\\n‚úÖ Embedding process complete!\")\nprint(\"üìä Final DataFrame shape:\", df_test_recon_D_logs.shape)\nprint(\"üß© Sample of new columns added:\")\nprint([col for col in df_test_recon_D_logs.columns if col.startswith(\"deb_emb_\")][:10])\n\n# Optional ‚Äî sanity check for alignment\nid_check = df_test_recon_D_logs[\"id\"].equals(df_test_recon_logs[\"id\"])\nprint(f\"üîç ID alignment check passed? {id_check}\")\n\n# Preview\ndisplay(df_test_recon_D_logs.head(2))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def merge_agg_and_deberta(df_agg, df_recon_D):\n    \"\"\"\n    General-purpose merge of aggregated event-level logs and reconstructed essay\n    datasets (with DeBERTa embeddings).\n\n    Parameters\n    ----------\n    df_agg : pd.DataFrame\n        Aggregated logs DataFrame (e.g., df_train_agg_logs or df_test_agg_logs)\n    df_recon_D : pd.DataFrame\n        Reconstructed essay DataFrame with linguistic + DeBERTa embeddings\n        (e.g., df_train_recon_D_logs or df_test_recon_D_logs)\n\n    Returns\n    -------\n    pd.DataFrame\n        Merged dataset (e.g., df_train_full or df_test_full)\n    \"\"\"\n\n    print(\"\\n==========================================================\")\n    print(\"üöÄ FINAL MERGE: Aggregated Logs + Reconstructed DeBERTa Essays\")\n    print(\"==========================================================\\n\")\n\n    # --- Make copies to avoid in-place modification ---\n    df1 = df_agg.copy()\n    df2 = df_recon_D.copy()\n\n    # --- Step 0: Sanity check for 'id' column existence ---\n    for name, df in zip([\"Aggregated logs\", \"Reconstructed + DeBERTa\"], [df1, df2]):\n        if \"id\" not in df.columns:\n            raise KeyError(f\"‚ùå '{name}' missing 'id' column!\")\n        print(f\"‚úÖ {name} shape: {df.shape}\")\n    print()\n\n    # ==========================================================\n    # STEP 1: Check essay_text alignment (optional, if exists)\n    # ==========================================================\n    if \"essay_text\" in df1.columns and \"essay_text\" in df2.columns:\n        mismatch_mask = df1.set_index(\"id\")[\"essay_text\"] != df2.set_index(\"id\")[\"essay_text\"]\n        mismatch_count = mismatch_mask.sum()\n        if mismatch_count == 0:\n            print(\"‚úÖ Essay text perfectly aligned ‚Äî using ['id', 'essay_text'] as merge keys.\")\n            join_cols = [\"id\", \"essay_text\"]\n        else:\n            print(f\"‚ö†Ô∏è Essay text mismatch in {mismatch_count} rows ‚Äî using 'id' only.\")\n            join_cols = [\"id\"]\n    else:\n        print(\"‚öôÔ∏è Using 'id' as merge key (no essay_text overlap).\")\n        join_cols = [\"id\"]\n\n    # ==========================================================\n    # STEP 2: Perform merge\n    # ==========================================================\n    try:\n        df_full = pd.merge(df1, df2, on=join_cols, how=\"left\", validate=\"1:1\")\n        print(f\"üìé Merge successful on {join_cols}. Shape: {df_full.shape}\")\n    except Exception as e:\n        print(f\"‚ùå Merge on {join_cols} failed: {e}\")\n        print(\"üîÅ Retrying merge on 'id' only...\")\n        df_full = pd.merge(df1, df2, on=\"id\", how=\"left\", validate=\"1:1\")\n        print(f\"‚úÖ Fallback merge succeeded. Shape: {df_full.shape}\")\n\n    # ==========================================================\n    # STEP 3: Drop duplicate columns automatically\n    # ==========================================================\n    dupes = df_full.columns[df_full.columns.duplicated()]\n    if len(dupes) > 0:\n        print(f\"\\n‚ö†Ô∏è Found {len(dupes)} duplicate column names:\")\n        print(\"   üß©\", list(dupes[:10]), \"...\" if len(dupes) > 10 else \"\")\n        df_full = df_full.loc[:, ~df_full.columns.duplicated()]\n        print(f\"üßπ Duplicates removed. Final shape: {df_full.shape}\")\n    else:\n        print(\"\\n‚úÖ No duplicate columns detected in merged dataset.\")\n\n    # ==========================================================\n    # STEP 4: Alignment check\n    # ==========================================================\n    same_ids = df_full[\"id\"].equals(df1[\"id\"])\n    missing_from_merge = df1[~df1[\"id\"].isin(df_full[\"id\"])]\n\n    print(\"\\nüîç Alignment verification:\")\n    print(f\" - ID alignment maintained? {same_ids}\")\n    print(f\" - Missing IDs after merge: {len(missing_from_merge)}\")\n\n    # ==========================================================\n    # ‚úÖ Final summary\n    # ==========================================================\n    print(\"\\nüéØ Merge completed successfully!\")\n    print(f\"üìä Final merged dataset shape: {df_full.shape}\")\n    print(f\"üîó Merge keys used: {join_cols}\")\n\n    # --- Optional preview ---\n    display(df_full.head(2))\n\n    # --- Optional essay text check ---\n    essay_cols = [col for col in df_full.columns if \"essay_text\" in col]\n    print(f\"\\nüß© Essay text-related columns: {essay_cols}\")\n\n    return df_full","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\ndef prepare_dataframe(df: pd.DataFrame, target_col: str = None):\n    \"\"\"\n    General-purpose DataFrame preparation function.\n    Performs:\n      - Ensures 'id' is a column (not index)\n      - Checks duplicate columns and duplicate IDs\n      - Optionally coerces target to numeric (if provided)\n      - Detects which columns have NaNs or Infs\n      - Replaces inf / -inf with NaN and fills NaN with 0 (features only)\n      - Leaves 'id' and 'essay_text' untouched\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Input DataFrame (train or test)\n    target_col : str or None\n        Target column name (e.g. 'score' for train).\n        If None, skips target-related checks.\n\n    Returns\n    -------\n    pd.DataFrame\n        Cleaned and ready DataFrame.\n    \"\"\"\n    print(\"üßπ Stage 0: Data Preparation & Sanity Checks\")\n\n    df = df.copy()\n\n    # ======================================================\n    # üÜî Ensure 'id' column exists\n    # ======================================================\n    if df.index.name == \"id\" or \"id\" not in df.columns:\n        if df.index.name == \"id\":\n            df = df.reset_index()\n            print(\"‚Ü™Ô∏è  Reset index: moved 'id' from index to column.\")\n    if \"id\" not in df.columns:\n        raise KeyError(\"‚ùå Missing required column: 'id'\")\n\n    # ======================================================\n    # ‚ö†Ô∏è Handle duplicates\n    # ======================================================\n    dup_cols = df.columns[df.columns.duplicated()].tolist()\n    if dup_cols:\n        print(f\"‚ö†Ô∏è Found duplicate columns (kept first occurrence): {dup_cols}\")\n        df = df.loc[:, ~df.columns.duplicated()]\n\n    dup_ids = df[\"id\"][df[\"id\"].duplicated()].unique()\n    if len(dup_ids) > 0:\n        print(f\"‚ö†Ô∏è Found {len(dup_ids)} duplicated IDs. Keeping first occurrence.\")\n        df = df.drop_duplicates(subset=[\"id\"], keep=\"first\")\n\n    # ======================================================\n    # üéØ Target column (optional)\n    # ======================================================\n    if target_col:\n        if target_col not in df.columns:\n            raise KeyError(f\"‚ùå Missing target column: '{target_col}'\")\n\n        before_non_numeric = df[target_col].dtype\n        df[target_col] = pd.to_numeric(df[target_col], errors=\"coerce\")\n        if str(before_non_numeric) != str(df[target_col].dtype):\n            print(f\"‚ÑπÔ∏è  Coerced '{target_col}' from {before_non_numeric} ‚Üí {df[target_col].dtype}\")\n\n    # ======================================================\n    # üß© Feature subset (exclude protected columns)\n    # ======================================================\n    protect_cols = {\"id\", \"essay_text\"}\n    if target_col:\n        protect_cols.add(target_col)\n    feature_cols = [c for c in df.columns if c not in protect_cols]\n\n    # ======================================================\n    # üîç Detect NaNs and Infs before cleaning\n    # ======================================================\n    inf_mask = np.isinf(df[feature_cols].to_numpy())\n    inf_cols = df[feature_cols].columns[np.any(inf_mask, axis=0)].tolist()\n\n    nan_mask = df[feature_cols].isna()\n    nan_cols = nan_mask.columns[nan_mask.any()].tolist()\n\n    inf_count = np.isinf(df[feature_cols].to_numpy()).sum()\n    nan_count = df[feature_cols].isna().sum().sum()\n\n    if inf_count or nan_count:\n        print(f\"‚ö†Ô∏è Detected issues in feature columns:\")\n        if inf_count:\n            print(f\"   ‚àû Infs: {inf_count} total, in {len(inf_cols)} columns.\")\n            print(f\"      ‚Ü≥ Columns with inf values: {inf_cols[:10]}{' ...' if len(inf_cols) > 10 else ''}\")\n        if nan_count:\n            print(f\"   üï≥Ô∏è NaNs: {nan_count} total, in {len(nan_cols)} columns.\")\n            nan_counts_per_col = df[feature_cols].isna().sum()\n            nan_counts_top = nan_counts_per_col[nan_counts_per_col > 0].sort_values(ascending=False).head(10)\n            print(\"      ‚Ü≥ Top NaN columns (count):\")\n            for col, cnt in nan_counts_top.items():\n                print(f\"         - {col}: {cnt}\")\n        print(\"‚Ü™Ô∏è  Cleaning features: replacing inf ‚Üí NaN ‚Üí 0\")\n    else:\n        print(\"‚úÖ No NaN or inf values detected in feature columns.\")\n\n    # ======================================================\n    # üßπ Replace inf and NaN\n    # ======================================================\n    df[feature_cols] = df[feature_cols].replace([np.inf, -np.inf], np.nan).fillna(0)\n\n    # ======================================================\n    # üìä Summary\n    # ======================================================\n    print(f\"\\n‚úÖ Data ready. Shape: {df.shape}\")\n    print(f\"üî¢ Features (excl. protected cols): {len(feature_cols)}\")\n    print(f\"üÜî Unique IDs: {df['id'].nunique()}  |  Rows: {len(df)}\")\n\n    if target_col:\n        print(f\"üéØ Target '{target_col}' ‚Äî min: {df[target_col].min():.4f}, max: {df[target_col].max():.4f}\")\n\n    return df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Merging the dataframes on the 'id' column\ndf_train_combined = df_train_tokenised.merge(df_train_SB, on='id', how='left')\ndf_train_combined = df_train_combined.merge(df_train_LDA, on='id', how='left')\ndf_train_combined_with_scores = df_train_combined.merge(df_train_scores, on='id', how='left')\n# Display the resulting dataframe\ndisplay(df_train_combined_with_scores)\n\n# Merging the dataframes on the 'id' column\ndf_test_combined = df_test_tokenised.merge(df_test_SB, on='id', how='left')\ndf_test_combined = df_test_combined.merge(df_test_LDA, on='id', how='left')\n# Display the resulting dataframe\ndisplay(df_test_combined)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Merge the two DataFrames on 'id' first\ndf_train_full_unchecked = pd.merge(df_train_agg_logs, df_train_recon_D_logs, on=\"id\", how=\"left\")\n\n# Apply the prepare_dataframe function to the merged DataFrame\ndf_train_combined_new = prepare_dataframe(df_train_full_unchecked.copy(), target_col=None)  # No target column for merged features\ndf_train_combined_with_scores_new = df_train_combined_new.merge(df_train_scores, on='id', how='left')\n\n# Display the resulting dataframe\ndisplay(df_train_combined_with_scores_new)\n\n# Merge the two DataFrames on 'id' first\ndf_test_full_unchecked = pd.merge(df_test_agg_logs, df_test_recon_D_logs, on=\"id\", how=\"left\")\n\n# List of the new columns to add\nnew_columns = ['count_cut', 'count_nonproduction', 'count_paste', 'count_replace']\n\n# Check if the columns already exist, and if not, add them with default value 0\nfor col in new_columns:\n    if col not in df_test_full_unchecked.columns:\n        df_test_full_unchecked[col] = 0  # Set all values to 0 for non-existing columns\n\n# Reorder columns in df_test_full_unchecked to match the order of df_train_full\ndf_test_full_unchecked = df_test_full_unchecked[df_train_combined_with_scores_new.drop(columns=['score']).columns]\n\n# Apply the prepare_dataframe function to the merged DataFrame\ndf_test_combined_new = prepare_dataframe(df_test_full_unchecked.copy(), target_col=None)  # No target column for merged features\n\n# ‚úÖ Final check and summary\ndisplay(df_test_combined_new)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3. Training","metadata":{}},{"cell_type":"markdown","source":"## Conduct Leave-One-Feature-Out (LOFO)","metadata":{}},{"cell_type":"code","source":"import torch\n\nif torch.cuda.is_available():\n    print(f\"GPU is available! Running on: {torch.cuda.get_device_name(0)}\")\nelse:\n    print(\"GPU not available, running on CPU.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\nimport xgboost as xgb\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom tqdm.notebook import tqdm # Use the notebook version for progress bar\nimport numpy as np\nimport copy # Import copy to ensure model is fresh for each training run\n\n# Assuming df_train_combined_with_scores_new is defined and loaded\n# Assuming tqdm.notebook is correctly imported\n\n# --- 1. Data Preparation ---\n# X: Features (drop 'id' and 'score' columns from df_train_combined_with_scores_new)\nX = df_train_combined_with_scores_new.drop(columns=['id', 'score']).copy()\ny = df_train_combined_with_scores_new['score']\n\n# üö® FIX: Drop non-numerical/text columns (e.g., 'essay_text').\n# XGBoost can only handle numerical or explicit categorical dtypes.\ntext_cols_to_drop = X.select_dtypes(include=['object']).columns\n\nif len(text_cols_to_drop) > 0:\n    print(f\"‚ö†Ô∏è Dropping non-numerical columns before training: {list(text_cols_to_drop)}\")\n    X = X.drop(columns=text_cols_to_drop)\nelse:\n    print(\"‚úÖ No object-type columns found to drop.\")\n\n\n# --- 2. Create Train and Validation Splits for LOFO ---\n# 80% for training the model, 20% for validating the feature importance.\nX_train, X_val, y_train, y_val = train_test_split(\n    X, y, test_size=0.20, random_state=42\n)\n\n# --- 3. Model Setup ---\n# Set up XGBoost with GPU support\n\ndef get_new_xgb_model():\n    \"\"\"Returns a new, un-fitted XGBoost model instance.\"\"\"\n    return xgb.XGBRegressor(\n        objective='reg:squarederror',\n        random_state=42,\n        n_estimators=100,\n        learning_rate=0.05,\n        # üí° FIX: Replace tree_method='gpu_hist' and gpu_id=0 with device='cuda:0'\n        # tree_method='gpu_hist', # Redundant/deprecated when using device\n        # gpu_id=0,               # Deprecated, causes XGBoostError\n        device='cuda:0',          # Use the first GPU\n        n_jobs=-1\n    )\n\nmodel_baseline = get_new_xgb_model()\n\n# --- 4. Baseline Calculation ---\nprint(\"\\nTraining Baseline Model...\")\n\n# Fit the model with ALL features on the training split\nmodel_baseline.fit(X_train, y_train)\n\n# Predict and calculate Baseline MSE on the UNSEEN validation split\npreds_baseline = model_baseline.predict(X_val)\nbaseline_rmse = np.sqrt(mean_squared_error(y_val, preds_baseline))\nprint(f\"Baseline RMSE with all features (on validation set): {baseline_rmse:.6f}\")\n\n# --- 5. LOFO Feature Importance Calculation ---\nprint(\"\\nStarting Leave-One-Feature-Out (LOFO) analysis...\")\nfeature_importance = []\n\n# Loop through each feature, dropping it one at a time\nfor feature in tqdm(X_train.columns, desc=\"LOFO Progress\"):\n    # 5.1 Prepare data with one feature removed\n    X_train_temp = X_train.drop(columns=[feature])\n    X_val_temp = X_val.drop(columns=[feature])\n\n    # 5.2 Re-train a FRESH model without the feature\n    model_lofo = get_new_xgb_model()\n    model_lofo.fit(X_train_temp, y_train)\n\n    # 5.3 Predict and Calculate RMSE on the UNSEEN validation set\n    preds_temp = model_lofo.predict(X_val_temp)\n    rmse_temp = np.sqrt(mean_squared_error(y_val, preds_temp))  # Calculate RMSE instead of MSE\n\n    # 5.4 Calculate the difference (Positive difference means the feature was important)\n    rmse_diff = rmse_temp - baseline_rmse  # Calculate RMSE difference instead of MSE difference\n\n    feature_importance.append({\n        'feature': feature,\n        'rmse_when_dropped': rmse_temp,  # Store RMSE when dropped\n        'rmse_difference': rmse_diff  # Store RMSE difference\n    })\n\n# --- 6. Results Display ---\ndf_lofo = pd.DataFrame(feature_importance)\ndf_lofo = df_lofo.sort_values(by='rmse_difference', ascending=False).reset_index(drop=True)\n\n# Temporarily set options to ensure all 200 rows are displayed if they exist\npd.set_option('display.max_rows', 200)\npd.set_option('display.max_columns', None)\n\nprint(\"\\n--- LOFO Results ---\")\nprint(\"Features are ranked by 'mse_difference': A POSITIVE value means removing the feature WORSENED the score (it was important).\")\ndisplay(df_lofo.head(200))\n\n# Also print the list of the top 200 feature names without truncation\nfeats = df_lofo['feature'].head(200).tolist()\nprint(\"\\n--- Top 200 Feature Names (List) ---\")\n# Use pprint for clean list display\nimport pprint\npprint.pprint(feats)\n\n# Save the full LOFO results for later use\ndf_lofo.to_csv('lofo_feature_importance.csv', index=False)\nprint(\"\\nFull LOFO results saved to lofo_feature_importance.csv\")\n\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Multi-Seed CAT (Function)","metadata":{}},{"cell_type":"code","source":"'''\ndef run_catboost_multi_seed(\n    df,\n    seeds=range(5),\n    n_splits=5,\n    n_top=25,\n    verbose=True\n):\n    \"\"\"\n    üêà CatBoost Multi-Seed CV Trainer (GPU-adaptive, Stratified, OOF-enabled)\n    ------------------------------------------------------------------------\n    - StratifiedKFold on discrete essay score bins (0.5‚Äì6.0)\n    - Out-Of-Fold predictions for stacking\n    - Auto GPU/CPU detection\n    - Aggregates feature importances across seeds\n    \"\"\"\n    from catboost import CatBoostRegressor, Pool\n    import numpy as np, pandas as pd, gc, torch, time, warnings\n    from sklearn.model_selection import StratifiedKFold\n    from sklearn.metrics import mean_squared_error\n    from IPython.display import display\n\n    warnings.filterwarnings(\"ignore\")\n    start_time = time.time()\n\n    # =========================================================\n    # ‚öôÔ∏è Detect device\n    # =========================================================\n    device_type = \"GPU\" if torch.cuda.is_available() else \"CPU\"\n    print(f\"üíª Using {device_type}\")\n\n    # =========================================================\n    # üßπ Data prep\n    # =========================================================\n    df = df.copy()\n    y = df[\"score\"].astype(float).values\n    X = (\n        df.drop(columns=[\"id\", \"score\", \"essay_text\"], errors=\"ignore\")\n        .replace([np.inf, -np.inf], np.nan)\n        .fillna(0)\n    )\n    features = X.columns.tolist()\n    n_samples, n_features = X.shape\n    print(f\"üìä Loaded: {n_samples:,} samples √ó {n_features:,} features\\n\")\n\n    if n_features == 0:\n        raise ValueError(\"‚ùå No valid features found for CatBoost training.\")\n\n    # Bin labels for stratified folds\n    y_bins = (y * 2).astype(int)\n\n    # =========================================================\n    # üöÄ Multi-seed Stratified CV\n    # =========================================================\n    all_rmse, all_models, all_importances = [], [], []\n    oof_preds = np.zeros(len(X))\n\n    print(f\"üöÄ Starting CatBoost CV: {len(seeds)} seeds √ó {n_splits}-folds (stratified)\\n\")\n\n    for s_i, seed in enumerate(seeds, 1):\n        kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n        fold_rmse, fold_models = [], []\n        print(f\"üå± Seed {seed} ({s_i}/{len(seeds)})\")\n\n        for fold, (tr_idx, va_idx) in enumerate(kf.split(X, y_bins), 1):\n            fold_start = time.time()\n\n            train_pool = Pool(X.iloc[tr_idx], label=y[tr_idx])\n            val_pool   = Pool(X.iloc[va_idx], label=y[va_idx])\n\n            # ‚úÖ Fixed: removed rsm & subsample (not GPU-supported for RMSE)\n            model = CatBoostRegressor(\n                task_type=device_type,\n                loss_function=\"RMSE\",\n                learning_rate=0.01,\n                depth=6,\n                iterations=5000,\n                l2_leaf_reg=3,\n                random_seed=seed,\n                early_stopping_rounds=100,\n                verbose=0\n            )\n\n            model.fit(train_pool, eval_set=val_pool, verbose=200 if verbose else False)\n\n            preds = model.predict(val_pool)\n            oof_preds[va_idx] += preds / len(seeds)\n\n            rmse = mean_squared_error(y[va_idx], preds, squared=False)\n            fold_rmse.append(rmse)\n            fold_models.append(model)\n\n            print(f\"   ‚úÖ Fold {fold}/{n_splits}: RMSE={rmse:.4f} | BestIter={model.get_best_iteration()} | ‚è± {(time.time()-fold_start):.1f}s\")\n\n        mean_rmse, std_rmse = np.mean(fold_rmse), np.std(fold_rmse)\n        all_rmse.append(mean_rmse)\n        all_models.extend(fold_models)\n\n        imp_df = pd.DataFrame({\n            \"feature\": features,\n            \"importance\": model.get_feature_importance(),\n            \"seed\": seed\n        })\n        all_importances.append(imp_df)\n        gc.collect()\n\n        print(f\"üåæ Seed {seed} done ‚Üí RMSE={mean_rmse:.4f} ¬± {std_rmse:.4f}\\n\")\n\n    # =========================================================\n    # üßÆ Aggregate importances\n    # =========================================================\n    avg_imp = (\n        pd.concat(all_importances)\n        .groupby(\"feature\", as_index=False)[\"importance\"]\n        .mean()\n        .sort_values(\"importance\", ascending=False)\n        .reset_index(drop=True)\n    )\n\n    total_min = (time.time() - start_time) / 60\n    mean_rmse, std_rmse = np.mean(all_rmse), np.std(all_rmse)\n    print(f\"üèÅ Completed {len(seeds)} seeds in {total_min:.2f} min\")\n    print(f\"üìâ Overall CV RMSE: {mean_rmse:.4f} ¬± {std_rmse:.4f}\\n\")\n\n    if verbose:\n        print(f\"üèÖ Top {n_top} Averaged Features:\")\n        display(avg_imp.head(n_top))\n\n    return {\n        \"all_models\": all_models,\n        \"oof_preds\": oof_preds,\n        \"all_rmse\": all_rmse,\n        \"mean_rmse\": mean_rmse,\n        \"std_rmse\": std_rmse,\n        \"feature_importance_avg\": avg_imp,\n        \"features\": features,\n        \"runtime_min\": total_min,\n        \"device_used\": device_type,\n    }\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Multi-Seed Training (Function)","metadata":{}},{"cell_type":"code","source":"'''\nimport time\n\ndef run_multiple_models(df_train, models=\"all\", seeds=range(10), folds=5, top_n=25):\n    \"\"\"\n    Run multiple machine learning models based on the given argument and display the results.\n\n    Parameters:\n    - df_train: The training dataframe.\n    - models: The models to run. Options: \"all\", \"lgb\", \"xgb\", \"cat\".\n    - seeds: The list of random seeds to use for model training.\n    - folds: The number of folds for cross-validation.\n    - top_n: The number of top features to consider.\n    \"\"\"\n    \n    models_dict = {\"lgb\": run_lightgbm_multi_seed,\n                   \"xgb\": run_xgboost_multi_seed,\n                   \"cat\": run_catboost_multi_seed}\n\n    print(\"‚ö° Starting multi-model training...\\n\")\n    start_all = time.time()\n    \n    # lgb Model\n    if models == \"all\" or \"lgb\" in models:\n        print(\"Training lgb...\")\n        res_lgb = run_lightgbm_multi_seed(df_train, seeds=seeds, n_splits=folds, n_top=top_n)\n        print(f\"üìâ lgb RMSE: {res_lgb['mean_rmse']:.4f}\")\n        \n        # Feature importance for lgb\n        imp_lgb = res_lgb['feature_importance_avg']\n        top25_features_lgb = imp_lgb.head(top_n)['feature'].tolist()\n        print(f\"lgb Top {top_n} features:\", top25_features_lgb)\n\n    # XGB Model\n    if models == \"all\" or \"xgb\" in models:\n        print(\"Training XGB...\")\n        res_xgb = run_xgboost_multi_seed(df_train, seeds=seeds, n_splits=folds, n_top=top_n)\n        print(f\"üìâ XGB RMSE: {res_xgb['mean_rmse']:.4f}\")\n        \n        # Feature importance for XGB\n        imp_xgb = res_xgb['feature_importance_avg']\n        top25_features_xgb  = imp_xgb.head(top_n)['feature'].tolist()\n        print(f\"XGB Top {top_n} features:\", top25_features_xgb)\n\n    # CAT Model\n    if models == \"all\" or \"cat\" in models:\n        print(\"Training CAT...\")\n        res_cat = run_catboost_multi_seed(df_train, seeds=seeds, n_splits=folds, n_top=top_n)\n        print(f\"üìâ CAT RMSE: {res_cat['mean_rmse']:.4f}\")\n        \n        # Feature importance for CAT\n        imp_cat = res_cat['feature_importance_avg']\n        top25_features_cat   = imp_cat.head(top_n)['feature'].tolist()\n        print(f\"CAT Top {top_n} features:\", top25_features_cat)\n\n    print(\"\\nüèÅ Training complete!\")\n    print(f\"‚è± Total time: {(time.time() - start_all)/60:.2f} min\")\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\ndf_train = df_train_full.copy()\nrun_multiple_models(df_train, models=\"cat\", seeds=range(1), folds=3, top_n=25)\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Extract Features","metadata":{}},{"cell_type":"code","source":"feats = ['paragraph_count',\n 'inter_key_median_lantency',\n 'feature 1048',\n 'feature 1049',\n 'feature 1120',\n 'feature 448',\n 'feature 449',\n 'feature 846',\n 'feature 847',\n 'feature 65',\n 'feature 138',\n 'feature 141',\n 'text_change_7_cnt',\n 'sent_word_count_median',\n 'feature 1023',\n 'feature 1031',\n 'feature 335',\n 'feature 364',\n 'feature 365',\n 'feature 789',\n 'feature 808',\n 'feature 809',\n 'R-bursts_median',\n 'Topic_4',\n 'cursor_position_max',\n 'feature 0',\n 'feature 1108',\n 'feature 1109',\n 'feature 1130',\n 'feature 126',\n 'feature 168',\n 'feature 185',\n 'feature 719',\n 'feature 720',\n 'feature 992',\n 'feature 993',\n 'paragraph_len_sum',\n 'sent_len_sum',\n 'sent_word_count_sum',\n 'word_count_max',\n 'word_count_std',\n 'word_len_count',\n 'word_len_sum',\n 'action_time_max',\n 'down_event_3_cnt',\n 'up_event_3_cnt',\n 'pauses_1_half_sec',\n 'paragraph_word_count_q2',\n 'product_to_keys',\n 'paragraph_len_max',\n 'text_change_6_cnt',\n 'feature 1124',\n 'feature 580',\n 'feature 200',\n 'feature 721',\n 'feature 994',\n 'feature 180',\n 'feature 556',\n 'mean_pause_time',\n 'cursor_position_std',\n 'word_len_q2',\n 'sent_word_count_q2',\n 'text_change_8_cnt',\n 'feature 905',\n 'feature 1115',\n 'feature 913',\n 'feature 288',\n 'sent_word_count_q7',\n 'feature 179',\n 'feature 47',\n 'feature 55',\n 'feature 908',\n 'text_change',\n 'paragraph_word_count_q7',\n 'down_event_12_cnt',\n 'up_event_12_cnt',\n 'feature 376',\n 'feature 377',\n 'feature 814',\n 'feature 76',\n 'feature 1075',\n 'feature 1079',\n 'feature 605',\n 'feature 614',\n 'feature 622',\n 'feature 923',\n 'feature 929',\n 'feature 934',\n 'action_time_std',\n 'feature 480',\n 'sent_word_count_first',\n 'sent_len_q2',\n 'feature 319',\n 'text_change_12_cnt',\n 'activity_1_cnt',\n 'down_event_2_cnt',\n 'up_event_2_cnt',\n 'feature 38',\n 'feature 43',\n 'feature 46',\n 'feature 9',\n 'word_len_q7',\n 'feature 336',\n 'paragraph_len_first',\n 'activity_3_cnt',\n 'feature 1063',\n 'feature 521',\n 'feature 522',\n 'feature 882',\n 'feature 883',\n 'feature 594',\n 'feature 1035',\n 'feature 1117',\n 'feature 823',\n 'feature 587',\n 'sent_len_mean',\n 'feature 893',\n 'feature 572',\n 'feature 1003',\n 'feature 741',\n 'text_change_11_cnt',\n 'feature 1080',\n 'feature 1081',\n 'feature 629',\n 'feature 630',\n 'feature 935',\n 'feature 936',\n 'keys_per_second',\n 'activity_0_cnt',\n 'down_event_0_cnt',\n 'text_change_0_cnt',\n 'up_event_0_cnt',\n 'down_event_10_cnt',\n 'up_event_10_cnt',\n 'feature 162',\n 'feature 163',\n 'feature 557',\n 'feature 92',\n 'feature 95',\n 'feature 14',\n 'feature 19',\n 'feature 235',\n 'feature 267',\n 'feature 276',\n 'feature 8',\n 'feature 1002',\n 'feature 1111',\n 'feature 740',\n 'word_len_median',\n 'sent_len_median',\n 'feature 252',\n 'feature 253',\n 'feature 593',\n 'feature 722',\n 'feature 995',\n 'feature 544',\n 'text_change_10_cnt',\n 'feature 508',\n 'feature 778',\n 'paragraph_word_count_min',\n 'feature 72',\n 'feature 75',\n 'R-bursts_std',\n 'feature 384',\n 'feature 817',\n 'feature 388',\n 'feature 897',\n 'feature 539',\n 'feature 1055',\n 'feature 863',\n 'feature 1066',\n 'std_pause_time',\n 'input_word_length_mean',\n 'paragraph_len_min',\n 'feature 389',\n 'feature 400',\n 'feature 418',\n 'feature 894',\n 'feature 79',\n 'feature 1123',\n 'feature 170',\n 'feature 78',\n 'Topic_2',\n 'total_pause_time',\n 'down_event_13_cnt',\n 'text_change_5_cnt',\n 'up_event_13_cnt',\n 'pauses_3_sec',\n 'word_len_max',\n 'P-bursts_count',\n 'feature 481',\n 'sent_word_count_mean',\n 'Topic_3',\n 'feature 1020',\n 'feature 1021',\n 'feature 419',\n 'feature 152',\n 'feature 153',\n 'feature 566',\n 'feature 84',\n 'feature 284',\n 'input_word_count',\n 'feature 410',\n 'feature 60',\n 'sent_len_first',\n 'feature 181',\n 'feature 417',\n 'word_count_mean',\n 'word_count_median',\n 'word_count_quantile',\n 'feature 383',\n 'feature 918',\n 'word_len_mean',\n 'down_event_9_cnt',\n 'up_event_9_cnt',\n 'feature 173',\n 'feature 1121',\n 'P-bursts_last',\n 'word_len_min',\n 'feature 1061',\n 'feature 1122',\n 'feature 519',\n 'feature 880',\n 'feature 85',\n 'feature 499',\n 'feature 507',\n 'feature 515',\n 'paragraph_word_count_mean',\n 'feature 898',\n 'word_len_first',\n 'input_word_length_max',\n 'feature 178',\n 'feature 1059',\n 'feature 873',\n 'feature 877',\n 'paragraph_word_count_max',\n 'feature 184',\n 'text_change_9_cnt',\n 'feature 174',\n 'paragraph_len_median',\n 'down_event_1_cnt',\n 'text_change_1_cnt',\n 'up_event_1_cnt',\n 'feature 1036',\n 'action_time_sum',\n 'down_event_14_cnt',\n 'up_event_14_cnt',\n 'feature 1091',\n 'feature 1092',\n 'feature 1127',\n 'feature 665',\n 'feature 666',\n 'feature 954',\n 'feature 955',\n 'input_word_length_std',\n 'sent_len_q7',\n 'down_event_8_cnt',\n 'text_change_3_cnt',\n 'up_event_8_cnt',\n 'down_time_min',\n 'up_time_min',\n 'feature 598',\n 'feature 503',\n 'feature 504',\n 'feature 876',\n 'R-bursts_mean',\n 'feature 1072',\n 'paragraph_word_count_last',\n 'activity_2_cnt',\n 'down_time_std',\n 'up_time_std',\n 'feature 1024',\n 'feature 790',\n 'Topic_5',\n 'paragraph_word_count_sum',\n 'paragraph_word_count_first',\n 'pauses_1_sec',\n 'R-bursts_max',\n 'inter_key_largest_lantency',\n 'paragraph_len_q7',\n 'pauses_2_sec',\n 'paragraph_len_mean',\n 'cursor_position_mean',\n 'cursor_position_median',\n 'cursor_position_quantile',\n 'feature 1062',\n 'feature 520',\n 'feature 881',\n 'down_event',\n 'up_event',\n 'down_event_11_cnt',\n 'up_event_11_cnt',\n 'paragraph_len_q2',\n 'paragraph_word_count_median',\n 'feature 127',\n 'text_change_4_cnt',\n 'paragraph_len_last',\n 'down_event_4_cnt',\n 'up_event_4_cnt',\n 'feature 169',\n 'Topic_0'] + ['_Topic_0','_Topic_1','_Topic_2','_Topic_3','_Topic_4','_Topic_5'] + [f'__Topic_{x}' for x in range(6)] ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"top25_features_cat_saved = [\n    'max_cursor',\n    'final_word_count',\n    'max_word_count',\n    'num_words',\n    'commas_per_sentence',\n    'q_tc_count',\n    'num_paragraphs',\n    'std_cursor',\n    'count_input',\n    'spaces_before_punct_per_100_tokens',\n    'commas_per_100_words',\n    'any_internal_punct_share',\n    'mean_cursor',\n    'body_para_mean_len',\n    'avg_internal_punct_per_sentence',\n    'words_per_second',\n    'dashes_count',\n    'q_overall_delta',\n    'time_per_word',\n    'std_word_count',\n    'multi_clause_sent_share',\n    'spaces_before_comma',\n    'double_spaces_after_eos',\n    'mean_sentence_len',\n    'deb_emb_424',\n]\n\nprint(f\"   CAT  ‚Üí {len(top25_features_cat_saved)} features\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('< Mapping >')\n\n# Select features and target (score)\nx = df_train_combined_with_scores.drop(['id', 'score'], axis=1)[feats]  # Features (excluding 'id' and 'score')\ny = df_train_combined_with_scores['score'].values  # Target\n\nprint(f'Number of features: {len(x.columns)}')\n\n## Test\ntest_ids = df_test_combined['id'].values  # Assuming 'df_test_full' holds the test data\ntestin_x = df_test_combined.drop(['id'], axis=1)[feats]  # Select test features based on the defined 'feats'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## t-distributed Stchastic Neighbour Embedding (TSNE)","metadata":{}},{"cell_type":"code","source":"from sklearn.manifold import TSNE\n#For Features Adding\ndf = pd.concat([x[feats],testin_x[feats]])\n\nt_sne = TSNE(n_components=2, random_state=42, perplexity=20, n_jobs=-1, verbose=True)\ndf_tsne = t_sne.fit_transform(df.fillna(0))\n\nplt.figure(figsize=(13,10))\nplt.scatter(df_tsne[:, 0], df_tsne[:, 1], cmap=\"jet\")\nplt.colorbar()\nplt.show()\n\nx['tsne_0'] = df_tsne[:x.shape[0],0]\nx['tsne_1'] = df_tsne[:x.shape[0],1]\n\ntestin_x['tsne_0'] = df_tsne[x.shape[0]:,0]\ntestin_x['tsne_1'] = df_tsne[x.shape[0]:,1]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.manifold import TSNE\n#For Features Adding\ndf = pd.concat([x[feats],testin_x[feats]])\n\nt_sne = TSNE(n_components=2, random_state=42, perplexity=50, n_jobs=-1, verbose=True)\ndf_tsne = t_sne.fit_transform(df.fillna(0))\n\nplt.figure(figsize=(13,10))\nplt.scatter(df_tsne[:, 0], df_tsne[:, 1], cmap=\"jet\")\nplt.colorbar()\nplt.show()\n\nx['_tsne_0'] = df_tsne[:x.shape[0],0]\nx['_tsne_1'] = df_tsne[:x.shape[0],1]\n\ntestin_x['_tsne_0'] = df_tsne[x.shape[0]:,0]\ntestin_x['_tsne_1'] = df_tsne[x.shape[0]:,1]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.manifold import TSNE\n# For Features Adding\ndf = pd.concat([x[feats],testin_x[feats]])\n\nt_sne = TSNE(n_components=2, random_state=42, perplexity=80, n_jobs=-1, verbose=True)\ndf_tsne = t_sne.fit_transform(df.fillna(0))\n\nplt.figure(figsize=(13,10))\nplt.scatter(df_tsne[:, 0], df_tsne[:, 1], cmap=\"jet\")\nplt.colorbar()\nplt.show()\n\nx['__tsne_0'] = df_tsne[:x.shape[0],0]\nx['__tsne_1'] = df_tsne[:x.shape[0],1]\n\ntestin_x['__tsne_0'] = df_tsne[x.shape[0]:,0]\ntestin_x['__tsne_1'] = df_tsne[x.shape[0]:,1]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## MultinomialNB","metadata":{}},{"cell_type":"code","source":"def train_valid_split(data_x, data_y, train_idx, valid_idx):\n    x_train = data_x[train_idx]\n    y_train = data_y[train_idx]\n    x_valid = data_x[valid_idx]\n    y_valid = data_y[valid_idx]\n    return x_train, y_train, x_valid, y_valid\n\n\n# Modify your evaluate function\ndef evaluate(data_x, data_y, model, n_splits=5, n_bags=1, test_x=None):\n    cv_results = np.zeros((len(data_x), pd.Series(data_y).nunique()))\n    test_results = np.zeros((len(test_x), pd.Series(data_y).nunique())) if test_x is not None else None\n\n    for bag in range(n_bags):\n        if n_bags == 1:\n            skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n        else:\n            skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=bag)\n        for i, (train_index, valid_index) in tqdm(enumerate(skf.split(data_x, data_y.astype(str)))):\n            train_x, train_y, valid_x, valid_y = train_valid_split(data_x, data_y, train_index, valid_index)\n            \n            model = MultinomialNB(alpha=1.0)\n            \n            # First training on the original training set\n            model.fit(train_x, train_y)\n\n            # Store the predictions\n            cv_predictions = model.predict_proba(valid_x)\n            cv_results[valid_index, :] = cv_predictions\n\n            # Predict on test set if available\n            if test_x is not None:\n                test_results += model.predict_proba(test_x) / n_splits\n\n    if test_x is not None:\n        return cv_results, test_results\n    else:\n        return cv_results, None","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from xgboost import XGBRegressor, XGBClassifier\n## from tabpfn import TabPFNClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostRegressor\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.neural_network import MLPClassifier   \nimport scipy.stats as stats\n\nvals = {4.0: 9,\n 3.5: 8,\n 4.5: 7,\n 3.0: 6,\n 2.5: 5,\n 5.0: 4,\n 5.5: 1,\n 2.0: 3,\n 1.5: 2,\n 6.0: 1,\n 1.0: 0,\n 0.5: 0}\n\n_y = pd.Series(y).map(vals)\n_y\n\nsolution = MultinomialNB(alpha=1.0)\np_comp = 100\npca = PCA(n_components=p_comp, random_state=42)\ndf = pd.concat([x,testin_x])\npca.fit(df.fillna(0))\n_x = pca.transform(x.fillna(0))**2\n_testin_x = pca.transform(testin_x.fillna(0))**2\n\noof_prob_2, oof_prob_test_2 = evaluate(_x.copy(), _y.copy(), solution, n_bags=1, test_x=_testin_x.copy(), n_splits=5)\noof_prob_2","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## MLP Classifer","metadata":{}},{"cell_type":"code","source":"def train_valid_split(data_x, data_y, train_idx, valid_idx):\n    x_train = data_x[train_idx]\n    y_train = data_y[train_idx]\n    x_valid = data_x[valid_idx]\n    y_valid = data_y[valid_idx]\n    return x_train, y_train, x_valid, y_valid\n\n\n# Modify your evaluate function\ndef evaluate(data_x, data_y, model, n_splits=5, n_bags=1, test_x=None):\n    cv_results = np.zeros((len(data_x), pd.Series(data_y).nunique()))\n    test_results = np.zeros((len(test_x), pd.Series(data_y).nunique())) if test_x is not None else None\n\n    for bag in range(n_bags):\n        if n_bags == 1:\n            skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n        else:\n            skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=bag)\n        for i, (train_index, valid_index) in tqdm(enumerate(skf.split(data_x, data_y.astype(str)))):\n            train_x, train_y, valid_x, valid_y = train_valid_split(data_x, data_y, train_index, valid_index)\n            \n            model = MLPClassifier(random_state=42)\n            \n            # First training on the original training set\n            model.fit(train_x, train_y)\n\n            # Store the predictions\n            cv_predictions = model.predict_proba(valid_x)\n            cv_results[valid_index, :] = cv_predictions\n\n            # Predict on test set if available\n            if test_x is not None:\n                test_results += model.predict_proba(test_x) / n_splits\n\n    if test_x is not None:\n        return cv_results, test_results\n    else:\n        return cv_results, None","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from xgboost import XGBRegressor, XGBClassifier\n## from tabpfn import TabPFNClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostRegressor\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.neural_network import MLPClassifier   \nimport scipy.stats as stats\n\nvals = {4.0: 9,\n 3.5: 8,\n 4.5: 7,\n 3.0: 6,\n 2.5: 5,\n 5.0: 4,\n 5.5: 1,\n 2.0: 3,\n 1.5: 2,\n 6.0: 1,\n 1.0: 0,\n 0.5: 0}\n\n_y = pd.Series(y).map(vals)\n_y\n\nsolution = MLPClassifier(random_state=42)\np_comp = 100\npca = PCA(n_components=p_comp, random_state=42)\ndf = pd.concat([x,testin_x])\npca.fit(df.fillna(0))\n_x = pca.transform(x.fillna(0))**2\n_testin_x = pca.transform(testin_x.fillna(0))**2\n\noof_prob_3, oof_prob_test_3 = evaluate(_x.copy(), _y.copy(), solution, n_bags=1, test_x=_testin_x.copy(), n_splits=5)\noof_prob_3","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Weighted Sum","metadata":{}},{"cell_type":"code","source":"_x = x.copy()\n_testin_x = testin_x.copy()\n## _x[[f'probs_tab_{i}' for i in range(10)]] = oof_prob_1.copy()\n## _testin_x[[f'probs_tab_{i}' for i in range(10)]] = oof_prob_test_1.copy()\n_x[[f'probs_nb_{i}' for i in range(10)]] = oof_prob_2.copy()\n_testin_x[[f'probs_nb_{i}' for i in range(10)]] = oof_prob_test_2.copy()\n_x[[f'probs_nn_{i}' for i in range(10)]] = oof_prob_3.copy()\n_testin_x[[f'probs_nn_{i}' for i in range(10)]] = oof_prob_test_3.copy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vals = {4.0: 9, 3.5: 8, 4.5: 7, 3.0: 6, 2.5: 5, 5.0: 4, 5.5: 1, 2.0: 3, 1.5: 2, 6.0: 1, 1.0: 0, 0.5: 0}\nInversemapper = {0:1, 1:6, 2: 1.5, 3:2, 4:5, 5: 2.5, 6:3, 7:4.5, 8:3.5, 9:4}\n\n## new feat\ndef f(x):\n    s = 0\n    for i, y in enumerate(x):\n        s += y*Inversemapper[i]\n    return s\nfor str_model in ['probs_nb_','probs_nn_']:  # Remove 'probs_tab_' if you no longer use it\n    _x[f'{str_model}w_sum'] = _x[[f'{str_model}{i}' for i in range(10)]].apply(f, axis=1)\n    _testin_x[f'{str_model}w_sum'] = _testin_x[[f'{str_model}{i}' for i in range(10)]].apply(f, axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Run XGBoost","metadata":{}},{"cell_type":"code","source":"\ndef train_valid_split(data_x, data_y, train_idx, valid_idx):\n    x_train = data_x.iloc[train_idx]\n    y_train = data_y[train_idx]\n    x_valid = data_x.iloc[valid_idx]\n    y_valid = data_y[valid_idx]\n    return x_train, y_train, x_valid, y_valid\n\n\n# Modify your evaluate function\ndef evaluate(data_x, data_y, model, n_splits=5, n_bags=1, test_x=None):\n    cv_results = np.zeros((len(data_x), n_bags))\n    test_results = np.zeros((len(test_x), n_bags)) if test_x is not None else None\n\n    for bag in range(n_bags):\n        if n_bags == 1:\n            skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n        else:\n            skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=bag)\n        for i, (train_index, valid_index) in tqdm(enumerate(skf.split(data_x, data_y.astype(str)))):\n            train_x, train_y, valid_x, valid_y = train_valid_split(data_x, data_y, train_index, valid_index)\n            \n                        \n            # First training on the original training set\n            model.fit(train_x, train_y)\n\n            # Store the predictions\n            cv_predictions = model.predict(valid_x)\n            cv_results[valid_index, bag] = cv_predictions\n\n            # Predict on test set if available\n            if test_x is not None:\n                test_results[:, bag] += model.predict(test_x) / n_splits\n\n    if test_x is not None:\n        return np.mean(cv_results, axis=1), np.mean(test_results, axis=1)\n    else:\n        return np.mean(cv_results, axis=1), None\n\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostRegressor\nfrom sklearn.svm import SVR\nimport scipy.stats as stats\n\nparam = {'n_estimators': 1000,\n         'learning_rate': 0.01,\n         'max_depth': 4,\n         'subsample':0.3,\n         'objective': 'reg:squarederror',\n         'random_state': 42}\n\nsolution = XGBRegressor(**param)\noof_3, test_results_xg_2 = evaluate(_x.copy(), y.copy(), solution, n_bags=1, test_x=_testin_x.copy(), n_splits=5)\nprint('XG CV RMSE: ',np.sqrt(mean_squared_error(y,oof_3)))\nspearman_correlation, p_value = stats.spearmanr(y, oof_3)\nprint('Spearman Correlation Coefficient: ', spearman_correlation)\n\n# XG CV RMSE:  0.5876518182062751\n# Spearman Correlation Coefficient:  0.822345159257767\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Output Best CatBoost Parameters (Run Once)","metadata":{}},{"cell_type":"code","source":"## !pip install optuna","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\nimport joblib, gc\nimport os\nimport numpy as np, pandas as pd\nimport optuna\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom lightgbm import LGBMRegressor\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor\nimport torch\nimport warnings\n\n# ============================================================\n# üö´ SUPPRESS ALL WARNINGS (global and library-specific)\n# ============================================================\nwarnings.filterwarnings(\"ignore\")\nos.environ[\"PYTHONWARNINGS\"] = \"ignore\"\nos.environ[\"XGBOOST_VERBOSITY\"] = \"0\"\nos.environ[\"CATBOOST_LOGGING_LEVEL\"] = \"Silent\"\n\nprint(\"üöÄ Retraining final (top-N) models on full training data (all warnings suppressed)...\")\n\ndf_train = df_train_full.copy()\n\n# --- Helper to restrict columns safely\ndef select_features(df_train, feats):\n    return df_train.loc[:, [f for f in feats if f in df_train.columns]]\n\n# --- Common clean-up\nX_full = df_train.drop(columns=['id', 'score', 'essay_text'], errors='ignore')\nX_full = X_full.replace([np.inf, -np.inf], np.nan).fillna(0)\ny_full = df_train['score'].values\n\n# ============================================================\n# Objective Function for CatBoost\n# ============================================================\ndef objective_cb(trial):\n    print(f\"\\nüöÄ Running trial {trial.number} for CatBoost...\")\n\n    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 0.1) if trial.number > 0 else 0.01\n    depth = trial.suggest_int('depth', 4, 12) if trial.number > 0 else 6\n    iterations = trial.suggest_int('iterations', 1000, 5000) if trial.number > 0 else 3000\n    l2_leaf_reg = trial.suggest_loguniform('l2_leaf_reg', 1e-5, 10.0) if trial.number > 0 else 0.1\n    bagging_temperature = trial.suggest_uniform('bagging_temperature', 0, 1.0) if trial.number > 0 else 0.2\n\n    X_train, X_val, y_train, y_val = train_test_split(X_full, y_full, test_size=0.2, random_state=42)\n\n    model = CatBoostRegressor(\n        iterations=iterations,\n        learning_rate=learning_rate,\n        depth=depth,\n        l2_leaf_reg=l2_leaf_reg,\n        bagging_temperature=bagging_temperature,\n        loss_function='RMSE',\n        random_seed=42,\n        verbose=False,\n        task_type='GPU' if torch.cuda.is_available() else 'CPU'\n    )\n\n    model.fit(X_train, y_train)\n    preds = model.predict(X_val)\n    rmse = mean_squared_error(y_val, preds, squared=False)\n    print(f\"Trial {trial.number}: RMSE = {rmse:.4f}\")\n    return rmse\n\n# ============================================================\n# Perform Optuna Hyperparameter Optimization for All Models\n# ============================================================\n\nstudy_cb = optuna.create_study(direction='minimize')\nstudy_cb.optimize(objective_cb, n_trials=10)\nprint(f\"Best CatBoost Params: {study_cb.best_params}\")\nprint(f\"Best RMSE: {study_cb.best_value}\")\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Save Model Parameters","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# Save best hyperparameters for each model\n# ============================================================\n\n# ============================================================\n# CatBoost Best Parameters\n# ============================================================\nbest_params_cat_saved = {\n    'learning_rate': 0.0049453015631225694,\n    'depth': 5,\n    'iterations': 1227,\n    'l2_leaf_reg': 3.9931762727996736e-05,\n    'bagging_temperature': 0.123929117769213\n}\n\n# Print out the saved parameters to check\nprint(f\"CatBoost Best Params: {best_params_cat_saved}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Kfold Train","metadata":{}},{"cell_type":"code","source":"\nimport numpy as np\nimport pandas as pd\nfrom catboost import CatBoostRegressor\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\n# ---------------------------------------------------------\n# Step 1: Load the full training dataset and prepare data\n# ---------------------------------------------------------\ndf_train = df_train_combined_with_scores_new.copy()\n\n# Prepare features and labels\nX_full = df_train.drop(columns=['id', 'score', 'essay_text'], errors='ignore')  # Use all features (excluding id, score, essay_text)\nX_full = X_full.replace([np.inf, -np.inf], np.nan).fillna(0)  # Handle missing values\ny_full = df_train['score'].values  # Target variable\n\n# ---------------------------------------------------------\n# Step 2: Use the top 25 features for CatBoost\n# ---------------------------------------------------------\ntop25_features_cat_saved = [\n    'max_cursor',\n    'final_word_count',\n    'max_word_count',\n    'num_words',\n    'commas_per_sentence',\n    'q_tc_count',\n    'num_paragraphs',\n    'std_cursor',\n    'count_input',\n    'spaces_before_punct_per_100_tokens',\n    'commas_per_100_words',\n    'any_internal_punct_share',\n    'mean_cursor',\n    'body_para_mean_len',\n    'avg_internal_punct_per_sentence',\n    'words_per_second',\n    'dashes_count',\n    'q_overall_delta',\n    'time_per_word',\n    'std_word_count',\n    'multi_clause_sent_share',\n    'spaces_before_comma',\n    'double_spaces_after_eos',\n    'mean_sentence_len',\n    'deb_emb_424',\n]\n\n# Use only the top 25 features for training\nX_full_top25 = X_full[top25_features_cat_saved]\n\n# ---------------------------------------------------------\n# Step 3: Define the best parameters for CatBoost (already saved)\n# ---------------------------------------------------------\nbest_params_cat = {\n    'iterations': 1227,\n    'depth': 5,\n    'learning_rate': 0.0049453015631225694,\n    'l2_leaf_reg': 3.9931762727996736e-05,\n    'bagging_temperature': 0.123929117769213,\n    'loss_function': 'RMSE',\n    'random_seed': 42,\n    'task_type': 'GPU',  # Use GPU for faster training if available\n    'verbose': 100,\n    'early_stopping_rounds': 100\n}\n\n# ---------------------------------------------------------\n# Step 4: KFold cross-validation to get OOF predictions\n# ---------------------------------------------------------\noof_preds_cat = np.zeros(len(X_full_top25))  # Initialize OOF predictions array\nkf = KFold(n_splits=5, shuffle=True, random_state=42)  # 5-fold cross-validation\n\nfor train_idx, val_idx in kf.split(X_full_top25):\n    X_train, X_val = X_full_top25.iloc[train_idx], X_full_top25.iloc[val_idx]\n    y_train, y_val = y_full[train_idx], y_full[val_idx]\n\n    # Train the CatBoost model with the current fold's training data\n    cat_model = CatBoostRegressor(\n        **best_params_cat  # Use best saved parameters\n    )\n    \n    cat_model.fit(X_train, y_train, eval_set=(X_val, y_val))  # Training with eval_set for validation\n\n    # Make predictions on the validation set and store them in the OOF array\n    oof_preds_cat[val_idx] = cat_model.predict(X_val)\n\n# ---------------------------------------------------------\n# Step 5: Evaluate the CatBoost model with OOF predictions\n# ---------------------------------------------------------\ncat_rmse = np.sqrt(mean_squared_error(y_full, oof_preds_cat))  # RMSE for the model using OOF predictions\nprint(f\"CatBoost RMSE (OOF): {cat_rmse:.4f}\")\n\n# ---------------------------------------------------------\n# Step 6: Test Data Preparation and Prediction\n# ---------------------------------------------------------\ndf_test = df_test_combined_new.copy()  # Assuming the test set is available\n\n# Prepare test features\nX_test = df_test.drop(columns=['id', 'essay_text'], errors='ignore')\nX_test = X_test.replace([np.inf, -np.inf], np.nan).fillna(0)  # Handle missing values in the test set\n\n# Use only the top 25 features for the test set\nX_test_top25 = X_test[top25_features_cat_saved]\n\n# Get predictions from the CatBoost model on the test data\ncat_test_preds = cat_model.predict(X_test_top25)\n\n'''\n# ---------------------------------------------------------\n# Step 7: Prepare the Submission\n# ---------------------------------------------------------\nsubmission = pd.DataFrame({\n    'id': df_test['id'],  # Use the 'id' from the test data\n    'score': np.clip(cat_test_preds, 0.5, 6.0)  # Ensure the scores are clipped to a valid range [0.5, 6.0]\n})\n\n# Save the submission to a CSV file\nsubmission.to_csv('/kaggle/working/submission.csv', index=False)\nprint(\"Submission file generated.\")\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Ensemble","metadata":{}},{"cell_type":"code","source":"from catboost import CatBoostRegressor\nfrom sklearn.svm import SVR, LinearSVR\nfrom sklearn.linear_model import ElasticNet, Ridge, PassiveAggressiveRegressor, HuberRegressor, PoissonRegressor, BayesianRidge\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom scipy.optimize import minimize\nfrom sklearn.metrics import mean_squared_error\n\n# =======================================================\n# ‚úîÔ∏è 1) Two-model OOF matrices: XGB + CatBoost\n# =======================================================\n\nsubs = np.vstack([\n    oof_3,          # XGBoost OOF predictions\n    oof_preds_cat   # CatBoost OOF predictions\n])  # shape: (2, N)\n\nsubs = subs.T  # shape becomes (N, 2)\n\nprint(\"Shape of subs:\", subs.shape)\n\n# =======================================================\n# ‚úîÔ∏è 2) Optimize 2 weights\n# =======================================================\n\ndef weighted_rmse(weights, subs, y_true):\n    ensemble_pred = np.dot(subs, weights)\n    return np.sqrt(mean_squared_error(y_true, ensemble_pred))\n\ninitial_weights = [0.5, 0.5]\n\nconstraints = ({'type': 'eq', 'fun': lambda w: sum(w) - 1})\nbounds = [(0, 1), (0, 1)]\n\nopt_result = minimize(\n    weighted_rmse,\n    initial_weights,\n    args=(subs, y),\n    method='SLSQP',\n    bounds=bounds,\n    constraints=constraints\n)\n\noptimal_weights = opt_result.x\n\nprint(\"Optimal weights:\", optimal_weights)\nprint(\"Optimized RMSE:\", weighted_rmse(optimal_weights, subs, y))\n\n\n# =======================================================\n# ‚úîÔ∏è 3) Apply ensemble to TEST predictions\n# =======================================================\n\n# XGBoost test predictions\ntest_xgb = test_results_xg_2\n\n# CatBoost test predictions\ntest_cat = cat_test_preds\n\nensemble_test = (\n    test_xgb * optimal_weights[0] +\n    test_cat * optimal_weights[1]\n)\n\n# =======================================================\n# ‚úîÔ∏è 4) Prepare submission\n# =======================================================\n\nensemble_sub = pd.DataFrame({\n    'id': test_ids,\n    'score': ensemble_test\n})\n\nensemble_sub\n\nensemble_sub.to_csv('/kaggle/working/submission.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}